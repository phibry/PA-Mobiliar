---
output: pdf_document
---

## 2. Theory

It is assumed that the reader of this paper already has basic knowledge of the mathematical principles of time series analysis. Therefore, this section will only briefly describe the mathematical models and processes.

### 2.1. Time-Series

Almost anything with a data point to a given timestamp can be named a time-series. The monthly gross domestic product, the weekly US gasoline production, or daily stock price movements. In this paper lies the focus of the analysis of financial time series. Due to trades often only take place during the week, there are gaps in the time series on the weekends, an exception would be the trading of cryptocurrencies like Bitcoin which are also tradeable at the weekends.

A series of data points with more or less equidistant time points $t$ with the sample length of $T$, is called a time-series $x_{t}, t=1,...,T$ [@eco1]. The analysis of a time-series $x_{t}$ involves creating a reasonable model that can be utilized to perform forecast predictions.

#### 2.1.1. Stationarity {#stationarity}

&nbsp;

In order to fit a suitable model with a given time series $x_{t}$, the assumptions of stationarity must be met. In this practical application, only the following weak-stationarity properties are required.

\begin{equation}
  \label{eq:mean}
  E[x_{t}]=\mu
\end{equation}

\begin{equation}
  \label{eq:var}
  Var(x_{t})=\sigma_{x}^{2}
\end{equation}

\begin{equation}
  \label{eq:cov}
  Cov(x_{t}, x_{t-k})=R(k)
\end{equation}

Many financial time-series are subject to shift, trends or changing volatility. In figure \ref{fig:chap2.1} are the stock prices of Alphabet Inc Class A (Google) visualized. This time-series shows a clear upwards drift and towards the end the volatility increases.

```{r chap2.1, echo=FALSE, fig.cap="Visualization of the adjusted prices of the Alphabet Inc Class A Stock."}
GOOGL <- getSymbols("GOOGL", auto.assign=F)
g.adj <- GOOGL[,6]
par(mfrow=c(1,1))
plot(g.adj, main="Adjusted Prices ~ Google")
```
\newpage

To improve the violated properties the first difference can be applied and additionally a logarithmic transformation can be performed [@slide_eco3_1]. The log-returns transformation can only performed to strict positive data.

$$\mathrm{LogReturn} = \mathrm{log}(x_{t})-\mathrm{log}(x_{t-1})$$

The result is the so-called log-returns.

```{r chap2.2, echo=FALSE, fig.cap="Visualization of the Log-Returns"}
g.adj.lr <- na.exclude(diff(log(g.adj)))
plot(g.adj.lr, main="Log-Returns of Adj. Prices ~ Google")
```
Applying the transformation to the data causes the drift to disappear, but the series still contains stronger and weaker volatile phases. This effect often occurs in non-stationary financial data and is called volatility cluster. This special property is used for the modelling of forecast models, which will be discussed in chapter [2.2](#models-section).

#### 2.1.2. Autocorrelation

&nbsp;

The autocorrelation function (ACF) reveals how the correlation between any two data points of the time series changes as their separation changes [@acf]. More precisely, acf measures the dependence between $x_{t}$ and $x_{t \pm k}$ at lag $k$. The partial autocorrelation (PACF) measures the dependency between $x_{t}$ and $x_{t-k}$ at lag $k$ [@eco1]. For stationary time series, ACF can be used to identify the model order of a MA-process, PACF for AR-processes.

In the following figure \ref{fig:chap2.3} are acf's of the non-stationary adjusted Google stock and their log-returns visualized. Both graphics show the typical pattern of a non-stationary time series. The left plot shows the dependence structure of the time series. This means that it takes a long time until the series changes. Often a large value is followed by another large value, which indicates a strong trend. This property of the series can be seen in figure \ref{fig:chap2.1}. In the right plot the acf of the log-returns are visualized. Remember figure \ref{fig:chap2.2}, there is no trend visible and therefore no long dependency structures are visible, only slight dependencies can be seen.

In the following section [2.2.](#models-section) the characteristics of the autocorrelation function can be used for the verification of ARIMA and ARCH-processes.

```{r chap2.3, echo=FALSE, fig.cap="Acf and Pacf of the LogReturns of Google"}
par(mfrow=c(1,2))
chart.ACF(g.adj, maxlag=20, main="Adjusted Prices ~ Google")
chart.ACF(g.adj.lr, maxlag=20, main="LogReturns ~ Google")
```

\newpage

### 2.2. Models {#models-section}

The following processes are used to determine certain properties and characteristics of a time series so that they are transformed into a model. The goal is to fit the time series as well as possible in order to create reliable forecasts.

#### 2.2.1. ARIMA

&nbsp;

An ARIMA(*p*,*d*,*q*) process is defined as follows.

\begin{equation} \label{eq:arima}
  x_{t}=c+a_{1}x_{t-1}+...+a_{p}x_{t-p}+\epsilon_{t}+b_{1}\epsilon_{t-1}+...+b_{q}\epsilon_{t-q}
\end{equation}

- $p$ and $q$ are the AR- and MA-model orders
- $a$ and $b$ are the AR- and MA-model parameters
- $d$ is the differential parameter
- $\epsilon_t$ is a white noise sequence
- $x_t$ is the given data $x_{1},...,x_{T}$

The mean of an ARIMA-process can be computed as: 

$$\mu=\frac{c}{1-a_{1}-...-a_{p}}$$
ARIMA processes can be divided into 4 different models Choosing a model that best represents the time series is a difficult task. The goal is to find the best possible model with as few parameters as possible.

The previously introduced ACF and PACF can help to choose simple models. Provided that the time series is stationary, the model orders can be determined directly. For an AR(*p*)-process (ARIMA(*p*,*0*,*0*)), the ACF plot will gradually decrease and simultaneously the PACF should have a sharp drop after *p* significant lags. For an MA(*q*)-process (ARIMA(*0*,*0*.*q*)) the opposite is true, the ACF should show a sharp drop after a certain *q* number of lags while PACF should show a gradual decreasing trend. If both ACF and PACF show a gradual decreasing pattern, then the ARMA(*p*,*0*,*q*)-process should be considered for modeling [@arima].



#### 2.2.2. ARCH & GARCH {#arch-garch-section}

&nbsp;

The volatility clustering mentioned in section [2.1.1](#stationarity) can be handled with an auto-regressive conditional heteroscedastic process.

\begin{align} \label{eq:arch}
  \epsilon_{t} &= \mathrm{log}(x_{t})-\mathrm{log}(x_{t-1}) \nonumber \\
  \epsilon_{t} &= \sigma_{t}u_{t} \\
  \sigma_{t}^{2} &=c \sigma^{2}+\sum_{k=1}^{m}\beta_{k}\epsilon_{t-k}^{2} \nonumber
\end{align}

with:

- $x_{t}$ is the original data (often non-stationary)
- $\epsilon_{t}$ is the stationary log-return
- $u_{t}$ is independent and identically distributed (iid) and standardized random variable
- $\sigma^{2}$ is the unconditional variance of the process $\epsilon_{t}$.
- $\sigma_{t}^{2}$ is the conditional variance of the process $\epsilon_{t}$.

The ARCH-process can be generalized by adding the lagged conditional variances to the equation \ref{eq:arch}.

\begin{align} \label{eq:garch}
  \epsilon_{t} &= \mathrm{log}(x_{t})-\mathrm{log}(x_{t-1}) \nonumber \\
  \epsilon_{t} &= \sigma_{t}u_{t} \\
  \sigma_{t}^{2} &=c \sigma^{2}+\sum_{j=1}^{n}\alpha_{j}\sigma_{t-j}^{2}+\sum_{k=1}^{m}\beta_{k}\epsilon_{t-k}^{2} \nonumber
\end{align}

#### 2.2.3. ARIMA-GARCH

&nbsp;

Another process is the combination of ARIMA and GARCH processes.

\begin{eqnarray}
y_{t}&=&\mu + a_{1}y_{t-1}+...+a_{p}y_{t-p}+\epsilon_{t}+b_{1}\epsilon_{t-1}+...+b_{q}\epsilon_{t-q} \label{eq:arima-garch1} \\
\epsilon_{t}&=&\sigma_{t}u_{t} \nonumber \\
\sigma_{t}^{2}&=&c\sigma^{2}+\sum_{j=1}^{n}\alpha_{j}\sigma_{t-j}^{2}+\sum_{k=1}^{m}\beta_{k}\epsilon_{t-k}^{2} \label{eq:arima-garch2}
\end{eqnarray}

Is called the mean-equation \ref{eq:arima-garch1}

Is called the variance-equation \ref{eq:arima-garch2}

[@eco2]

\newpage

### 2.3. Moving Average Filters

moving average filters are basically used to identify trends and smooth out price fluctuations. As a commonly used tool moving average filters are very simple in its usage, historical data was summarized and divided bi the length of the filter. The actual challenge in using Moving average filters is to figure out which length of the filter brings the most useful information. 


#### 2.3.1. Equally-weighted Moving Average

&nbsp;

SMA stands for Simple Moving Average which. depending on the length of the filter(l) l obervations since the last noted observation will be considered. theyre getting summarized and divided by the filterlength equals the EqMA. For every timestep a new observation is considered and the last one eliminated.

EqMA

\begin{equation}
  \label{eq:eqma}
  y_{t}=\frac{1}{L}\sum_{k=0}^{L-1}x_{t-k}
\end{equation}

#### 2.3.2. Exponentially-weighted Moving Average

&nbsp;

Since not all observations are having the same influence of future value we can apply a weight to past observations. One method will be exponentially weighted Moving average. So we chose an optimal parameter to give past observations weights decreasing by alpha

A skillfull trader chose an optimal to increase the performace of the measurment. Weights could also be given individually by adding a w vector to the filter

EMA

\begin{equation}
  \label{eq:ema}
  y_{t}=\frac{1}{\sum_{k=0}^{m}\alpha^{k}}\sum_{k=0}^{L-1}\alpha^{k}x_{t-k}
\end{equation}

#### 2.3.3. Moving Average Crossings

&nbsp;

Moving average crossings are basically just different MA's with different lengths applied to a timeseries. The points the filters then cross will be used as a trading signal to go long, short or hold.

An easy example of Ma average crossings with an eqMA and and SMA is visualized in }


```{r,chap2.3.3 , echo=FALSE,warning=FALSE, message=FALSE}
par(mfrow=c(1,2))#\ref{fig:chap2.1
n1=30
n2=200
chartSeries(GOOGL,type="line",subset="2007::2009",
            theme=chartTheme("white"))
addSMA(n=n1,on=1,col = "blue")
addSMA(n=n2,on=1,col = "red")
```
ciao anlksjdläjkayFOUJISDOUFJSD

schssehs
\newpage

### 2.4. Real Strength Index

### 2.5. Sharpe Ratio

### 2.6. Carry

### 2.7. Value

Try other github branch