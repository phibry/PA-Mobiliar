---
output: pdf_document
geometry: margin = 1in
---
```{r, echo=FALSE, fig.cap="Visualization of the 4 indexes",message=F,warning=F}


#########################################attenzione !!!  this chunk is only to knit this chapter can be deleted afterwards####
### bei m ir geht der knit nicht mit ljungplotgarch darum werden hier die funktionen nicht aufgerufen
#load("data/data_mobi")
#source("add/libraries.R")

#ind <- data[,1:4]
#int <- data[,5:12]
```






## 2. Theory

It is assumed that the reader of this paper already has basic knowledge of the mathematical principles of time series analysis. Therefore, this section will only briefly describe the mathematical models and processes. This theory part is justified by the presentation of the different models and goes beyond the applied models in methodology.

### 2.1. Time-Series

Almost anything with a data point to a given timestamp can be named a time-series. The monthly gross domestic product, the weekly US gasoline production, or daily stock price movements. In this paper lies the focus of the analysis of financial time series. Due to trades often only take place during the week, there are gaps in the time series on the weekends, an exception would be the trading of cryptocurrencies like Bitcoin which are also tradeable at the weekends.

A series of data points with more or less equidistant time points $t$ with the sample length of $T$, is called a time-series $x_{t}, t=1,...,T$ [@eco1]. The analysis of a time-series $x_{t}$ involves creating a reasonable model that can be utilized to perform forecast predictions.

#### 2.1.1. Stationarity {#stationarity}

&nbsp;

In order to fit a suitable model with a given time series $x_{t}$, the assumptions of stationarity must be met. In this practical application, only the following weak-stationarity properties are required.

\begin{equation}
  \label{eq:mean}
  E[x_{t}]=\mu
\end{equation}

\begin{equation}
  \label{eq:var}
  Var(x_{t})=\sigma_{x}^{2}
\end{equation}

\begin{equation}
  \label{eq:cov}
  Cov(x_{t}, x_{t-k})=R(k)
\end{equation}

Many financial time-series are subject to shift, trends or changing volatility. In figure \ref{fig:chap2.1} are the stock prices of Alphabet Inc Class A (Google) visualized. This time-series shows a clear upwards drift and towards the end the volatility increases.

```{r chap2.1, echo=FALSE, fig.cap="Visualization of the adjusted prices of the Alphabet Inc Class A Stock."}
GOOGL <- getSymbols("GOOGL", auto.assign=F)
g.adj <- GOOGL[,6]
par(mfrow=c(1,1))
plot(g.adj, main="Adjusted Prices ~ Google")
```
\newpage

To improve the violated properties the first difference can be applied and additionally a logarithmic transformation can be performed [@slide_eco3_1]. The log-returns transformation can only performed to strict positive data.

$$\mathrm{LogReturn} = \mathrm{log}(x_{t})-\mathrm{log}(x_{t-1})$$

The result is the so-called log-returns.

```{r chap2.2, echo=FALSE, fig.cap="Visualization of the Log-Returns"}
g.adj.lr <- na.exclude(diff(log(g.adj)))
plot(g.adj.lr, main="Log-Returns of Adj. Prices ~ Google")
```
Applying the transformation to the data causes the drift to disappear, but the series still contains stronger and weaker volatile phases. This effect often occurs in non-stationary financial data and is called volatility cluster. This special property is used for the modeling of forecast models, which will be discussed in chapter [2.2](#models-section).

In the following examples, we will only work with a section of the time series, as it often makes no sense to look long into the past. The further a value lies in the past, the smaller its influence on a future value will be.

#### 2.1.2. Autocorrelation

&nbsp;

The autocorrelation function (ACF) reveals how the correlation between any two data points of the time series changes as their separation changes [@acf]. More precisely, acf measures the dependence between $x_{t}$ and $x_{t \pm k}$ at lag $k$. The partial autocorrelation (PACF) measures the dependency between $x_{t}$ and $x_{t-k}$ at lag $k$ [@eco1]. For stationary time series, ACF can be used to identify the model order of a MA-process, PACF for AR-processes.

In the following figure \ref{fig:chap2.1.2} are ACF and PACF of the non-stationary adjusted Google stock visualized. Both graphics show the typical pattern of a non-stationary time series. The plot above shows the dependence structure of the time series. This means that it takes a long time until the series changes. Often a large value is followed by another large value, which indicates a strong trend. This property of the series can be seen in figure \ref{fig:chap2.1} as the long upward drift. The plot below indicates a significant partial autocorrelation at lag $k=1$.

In the following section [2.2.](#models-section) the characteristics of the autocorrelation function can be used for the verification of ARIMA and ARCH-processes.

```{r chap2.1.2, echo=FALSE, fig.cap="Acf and Pacf of the Adjusted Prizes of Google."}
par(mfrow=c(1,1))
chart.ACFplus(g.adj, maxlag=20, main="Adjusted Prices ~ Google")
```

\newpage

### 2.2. Models {#models-section}

The following processes are used to determine certain properties and characteristics of a time series so that they are transformed into a model. The goal is to fit the time series as well as possible in order to create reliable forecasts.

#### 2.2.1. ARIMA

&nbsp;

An ARIMA(*p*,*d*,*q*) process is defined as follows.

\begin{equation} \label{eq:arima}
  x_{t}=c+a_{1}x_{t-1}+...+a_{p}x_{t-p}+\epsilon_{t}+b_{1}\epsilon_{t-1}+...+b_{q}\epsilon_{t-q}
\end{equation}

- $p$ and $q$ are the AR- and MA-model orders
- $a$ and $b$ are the AR- and MA-model parameters
- $d$ is the differential parameter
- $\epsilon_t$ is a white noise sequence
- $x_t$ is the given data $x_{1},...,x_{T}$

The mean of an ARIMA-process can be computed as: 

$$\mu=\frac{c}{1-a_{1}-...-a_{p}}$$

ARIMA processes can be divided into 4 different models. Choosing a model that best represents the time series is a difficult task. The goal is to find the best possible model with as few parameters as possible.

The previously introduced ACF and PACF can help to determine the orders of simple models. Provided that the time series is stationary, the model orders can be determined directly. For an AR(*p*)-process (ARIMA(*p*,*0*,*0*)), the ACF plot will gradually decrease and simultaneously the PACF should have a sharp drop after *p* significant lags. For an MA(*q*)-process (ARIMA(*0*,*0*.*q*)) the opposite is true, the ACF should show a sharp drop after a certain *q* number of lags while PACF should show a gradual decreasing trend. If both ACF and PACF show a gradual decreasing pattern, then the ARIMA(*p*,*0*,*q*)-process should be considered for modeling [@arima]. If the time series is not stationary, differentiation can be considered (ARIMA(*p*,*d*,*q*)).

\newpage

The application of an analysis method to Google prices finds an ARIMA(*1*,*1*,*0*) as the optimal model. This makes sense if you look back at figure \ref{fig:chap2.1.2}. Long dependency structures in the ACF plot indicating an AR(*p*) process and at the same time after lag=$1$=*p* the PACF has a strong drop. The differential operator *d*=$1$ transforms the non-stationary series into a stationary one.

To convince yourself of the quality of the model, you can use the Ljung-Box statistics shown in figure \ref{fig:chap2.2.1_1}. For the lags where the p-values are above the 5% line, the forecasts are reliable. Here the values from the 6 lag are below the significant line, but since one only wants to make short-term forecasts (for example 1 day into the future), the lag is sufficient up to 5. If you want to forecast further than 5 days into the future, you might have to adjust the ARIMA model.

```{r chap2.2.1_1, echo=FALSE, fig.cap="Ljung-Box statistic of Google log returns.", fig.keep = 'last'}
x <- na.exclude(g.adj["2010-01-01/"])

# find best model
fit <- auto.arima(x)

# Plot LjungBox statistic
ljungplot(fit$residuals)
```

In figure \ref{fig:chap2.2.1_2} you can see the prediction of the model. The whole representation is shifted by one day so that one can compare the model with a true value.
The \textcolor{green}{green dot} is the actual value of the time series.
The \textcolor{red}{red dots} indicate the upper and lower 95% interval limits respectively. These indicate that a future value will be within this band. The \textcolor{blue}{blue dot} is the point forecast predicted by the model.

\newpage

```{r chap2.2.1_2, echo=FALSE, fig.cap="ARIMA-Forecast.", fig.keep = 'last'}
h <- 1

# For Visualization
in_sample <- x[1:(length(x)-h)]
out_sample <- x[((length(x)-h)+1):(length(x))]

# perform a forecast
fore <- forecast(fit, h=h)

# Create xts-Object for the forecast
fcast <- data.frame("LowerBand" = fore$lower[,2],
                    "UpperBand" = fore$upper[,2],
                    "PointForecast" = fore$mean)
rownames(fcast) <- index(out_sample)
fcast <- as.xts(fcast)
index(fcast) <- index(out_sample)


x[index(tail(x, h))] <- NA

ymin <- min(c(na.exclude(as.numeric(tail(x, 7))), as.numeric(fcast[,1])))
ymax <- max(c(na.exclude(as.numeric(tail(x, 7))), as.numeric(fcast[,3])))
plot(tail(x, 7), ylim=c(ymin-ymin*0.05, ymax+ymax*0.05), main="ARIMA(1,1,0)")
points(fcast[,1], col="red", pch=16)
points(fcast[,2], col="red", pch=16)
points(fcast[,3], col="blue", pch=16)
points(out_sample, col="green", pch=16)
```

#### 2.2.2. GARCH {#garch-section}

&nbsp;

The volatility clustering mentioned in section [2.1.1](#stationarity) can be handled with an generalized auto-regressive conditional heteroscedastic process.

\begin{align} \label{eq:garch}
  \epsilon_{t} &= \mathrm{log}(x_{t})-\mathrm{log}(x_{t-1}) \nonumber \\
  \epsilon_{t} &= \sigma_{t}u_{t} \\
  \sigma_{t}^{2} &=c \sigma^{2}+\sum_{j=1}^{n}\alpha_{j}\sigma_{t-j}^{2}+\sum_{k=1}^{m}\beta_{k}\epsilon_{t-k}^{2} \nonumber
\end{align}

with

- $x_{t}$ is the original data (often non-stationary)
- $\epsilon_{t}$ is the stationary log-return
- $u_{t}$ is independent and identically distributed (iid) and a standardized random variable
- $\sigma^{2}$ is the unconditional variance of the process $\epsilon_{t}$.
- $\sigma_{t}^{2}$ is the conditional variance of the process $\epsilon_{t}$.

With a GARCH(*n*,*m*)-process it is possible to model the volaclusters of a time series. The GARCH(*1*,*1*) model has become widely used in financial time series modeling and is implemented in most statistics and econometric software packages. Those models are favored over other stochastic volatility models by many economists due to their relatively simple implementation [@garch11].

\newpage

For an optimal model some conditions must be fulfilled. Suppose you want to model the Google time series with a GARCH(*1*,*1*).

In table \ref{tab:coeftable} the estimated coefficients of the process can be seen. The p-values are all lower than 0.05 and thus indicate that they are essential for the model. (Note: $\omega=c\sigma^{2}$)

```{r coeftable, echo=FALSE, message=FALSE}
g.subset.lr <- head(g.adj.lr, length(g.adj.lr)-100)
y.garch_11 <- garchFit(~garch(1,1), data=g.subset.lr, delta=2, include.delta=F, 
                       include.mean=F, trace=F)
r1 <- y.garch_11@fit$matcoef[,1]
r2 <- y.garch_11@fit$matcoef[,2]
r3 <- y.garch_11@fit$matcoef[,4]

paras <- data.frame(r1, r2, r3)
rownames(paras) <- c("$\\omega$", "$\\alpha_{1}$", "$\\beta_{1}$")
colnames(paras) <- c("Estimate", "Std. Error", "p-Value")

kable(paras, "latex", escape=F, booktabs = T, linesep="", caption="Coefficients GARCH(1,1).", digits=20)
```

The following parameter restrictions are also examined:

\begin{equation} \label{eq:para_restriction}
  c+\sum_{j=1}^{n}\alpha_{j}+\sum_{k=1}^{m}\beta_{k}=1
\end{equation}

with

$$c>0, \alpha_{k}\geq0, j=1,...,n, \beta_{k}\geq0, 1,...,m $$

To satisfy formula \ref{eq:para_restriction}, $c$ needs to be determined from $\omega$. First calculate the unconditional variance.

$$\sigma^{2} = \frac{\omega}{1-\alpha_{1}-\beta_{1}}$$

Calculate $c$ with:

$$c=\frac{\omega}{\sigma^{2}}$$

and then check for the restriction in \ref{eq:para_restriction}.

For the coefficients of GARCH(*1*,*1*) the restrictions are fulfilled. You can see that $c = 1-\alpha_{1}-\beta_{1}$. So this restriction can be determined easily with:

\begin{equation} \label{eq:para_moments_easy}
  \sum_{j=1}^{n}\alpha_{j}+\sum_{k=1}^{m}\beta_{k} < 1
\end{equation}

If the parameter restrictions are not fulfilled, complications may arise, the forecast of the conditional variance $\hat{\sigma}^{2}_{t}$ may diverge to the unconditional variance $\sigma^{2}$ of the process.

\newpage

Furthermore, the Ljung-Box statistics are important for the standardized residuals. Looking back at formula \ref{eq:garch} standardized residuals $u_{t}$ are proportional to the conditional volatilities $\sigma_{t}$, which should lead to the log returns $\epsilon_{t}$.  The conditional volatilities map the volacluster in the time series. To achieve the best possible model, one does not want to find these volacluster effects in the standardized residuals, but only in the conditional volatilities. The Ljung-Box statistics check this property. In figure \ref{fig:chap2.2.2} Ljung-Box statistics of the $\hat{u}_{t}$ and the $\hat{u}^{2}_{t}$ are shown.

```{r chap2.2.2, echo=FALSE, fig.cap="Ljung-Box statistic of the standardized residuals.", fig.keep = 'last'}
ljungplotGarch(y.garch_11@residuals, y.garch_11@sigma.t)
```

The plot shows reliable statistics for forecasts from the Google-GARCH(*1*,*1*) model. For all lags up to 20, the p-values are above the 5% line and thus hypothesis tests are discarded. However, if for a given lag=k the p-values would fall below the 5% line, then forecasts would only be reliable up to a forecast horizon k.

If one wants to improve the the standardized residuals $\hat{u}_{t}$ (if autocorrelations exists in the $\hat{u}_{t}$), an ARMA part would have to be added to the existing model (see [2.2.3](#arma-garch-section)). This can again be optimized with different model orders. If you want to improve the squared standardized residuals $\hat{u}^{2}_{t}$ (if volaclustering exists within the $\hat{u}^{2}_{t}$), then you should modify the GARCH model order.

Now an optimal model has been found and a forecast can be made. Since a GARCH(*n*,*m*) process is white noise sequence the expected value $E[\epsilon_{T+h} | \epsilon_{T},...,\epsilon_{1}]=\mu=0$ can be assumed (if the mean value in the fit object was also estimated and is significant, then the expected value is the estimated $\mu$).

Calculating the forecast variance is a recursive process. With increasing model order the calculation becomes more and more difficult. For this work the rather simple calculation for a GARCH(*1*,*1*) model is sufficient. One receives:

$$\hat{\sigma}_{T+h}^{2}=\omega+(\alpha_{1}+\beta_{1})^{2}\hat{\sigma}_{T+h-1}^{2}$$

If the parameter restriction from formula \ref{eq:para_moments_easy} is true, the forecast variance converges with the increasing forecast horizon to the unconditional variance of the process.

$$\hat{\sigma}_{T+h}^{2}=\frac{\omega}{1-\alpha_{1}-\beta_{1}}=\sigma^{2}$$

\newpage

The 95% forecast interval is calculated as follows:

$$E[\epsilon_{T+h} | \epsilon_{T},...,\epsilon_{1}]\pm 1.96\sqrt{\hat{\sigma}_{T+h}^{2}}$$

Figure \ref{fig:chap2.2.3} shows the GARCH(*1*,*1*) forecast for 20 days. The blue line represents the expected value of the time series. Reds are the two 95%-interval limits and green shows the actual values of the series.

```{r chap2.2.3, echo=FALSE, fig.cap="GARCH-Forecast.", fig.keep = 'last'}
sig <- sqrt(y.garch_11@fit$coef["omega"]/(1-y.garch_11@fit$coef["alpha1"]-y.garch_11@fit$coef["beta1"]))
names(sig) <- NULL

h <- 20
y.garch_11.pred <- predict(y.garch_11, n.ahead=h)



fcast_grach <- data.frame("MeanForecast" = y.garch_11.pred[,1],
                          "MeanError" = y.garch_11.pred[,2],
                          "Std.Deviation" = y.garch_11.pred[,3])
rownames(fcast_grach) <- index(tail(g.adj.lr["2010-01-01/"], h))
fcast_grach <- as.xts(fcast_grach)
index(fcast_grach) <- index(tail(g.adj.lr["2010-01-01/"], h))

par(mfrow=c(1,1))
plot(g.adj.lr["2020-01-01/"], main="GARCH(1,1)")
lines(tail(g.adj.lr["2020-01-01/"], h), col="green", lwd=1.5)
lines(fcast_grach[,1], col="blue", lwd=1.5)
lines(fcast_grach[,1] + 1.96 * fcast_grach[,3], col="red", lwd=1.5)
lines(fcast_grach[,1] - 1.96 * fcast_grach[,3], col="red", lwd=1.5)

lines(fcast_grach[,1] + 1.96 * sig, col="black", lwd=1, lty=2)
lines(fcast_grach[,1] - 1.96 * sig, col="black", lwd=1, lty=2)
```

You can see how the red lines are approaching the black dotted lines ($0 \pm 1.96*\sqrt{\sigma^{2}}$) as the forecast horizon increases. This means that the forecast variance converges to the unconditional variance and thus there is another proof for the satisfaction of the parameter restrictions.

#### 2.2.3. ARMA-GARCH {#arma-garch-section}

&nbsp;

If you want to adopt a GARCH model and you discover non-vanishing autocorrelations in the standardized residuals $\hat{u}_{t}$, the model can be extended with an ARMA part to counteract the autocorrelations.

\begin{eqnarray}
y_{t}&=&\mu + a_{1}y_{t-1}+...+a_{p}y_{t-p}+\epsilon_{t}+b_{1}\epsilon_{t-1}+...+b_{q}\epsilon_{t-q} \label{eq:arma-garch1} \\
\epsilon_{t}&=&\sigma_{t}u_{t} \nonumber \\
\sigma_{t}^{2}&=&c\sigma^{2}+\sum_{j=1}^{n}\alpha_{j}\sigma_{t-j}^{2}+\sum_{k=1}^{m}\beta_{k}\epsilon_{t-k}^{2} \label{eq:arma-garch2}
\end{eqnarray}

This model contains now 4 different model orders: *p*,*q*,*n* and *m*. Experience shows that for financial time series, a model order of *n*=*m*=$1$ and *p*,*q*$\leq 1$ is often sufficient to fit the data to an ARMA-GARCH model.

The equation \ref{eq:arma-garch1} is called the *mean-equation* and describes the conditional mean of the process, which is used to obtain optimal point forecast. The equation \ref{eq:arma-garch2} defines the variance of the process and is called *variance-equation*, which is used for the forecast error variance [@eco2].

\newpage

#### 2.2.4. M-GARCH

&nbsp;

The last model that is considered as a forecast model in this thesis is the so-called MGARCH model. Highly volatile phases indicate downturns. Market participants tend to oversell during these downturns. Overselling leads to inflated volumes and this in turn leads to inflated volatilities. This model can be helpful since it emphasizes recession and crisis dynamics.

\begin{align} \label{eq:mgarch}
  r_{t} &= \mathrm{log}(x_{t})-\mathrm{log}(x_{t-1}) \nonumber \\
  r_{t} &= \mu+e\sigma_{t}+\epsilon_{t} \\
  \epsilon_{t} &= \sigma_{t}u_{t} \nonumber
\end{align}

where

- $x_{t}$ is the original data (typically non-stationary)
- $r_{t}$ are the log-returns (stationary)
- $\mu$ is the long-term drift
- $\epsilon_{t}$ is a volacluster process (GARCH)
- $e$ is a constant (a parameter to be estimated), $e>0$ implies a larger expected return. $e<0$ would imply a smaller expected return. If $e=0$ then the MGARCH-effect vanishes [@slide_eco3_3].

For this model, the in-sample conditional standard deviations (volatilities) from any GARCH process are determined and the out-of-sample conditional standard deviation for obtaining forecasts of the future returns is then calculated by regression.

$$\hat{r}_{t+1}=\hat{\mu}+\hat{e}\hat{\sigma}_{t+1}$$

\newpage

### 2.3. Moving Average Filters

Moving average filters are basically used to identify trends and smooth out price fluctuations. As a commonly used tool, moving average filters are very simple in their usage, historical data from a timeframe L gets summarized and divided by the length of the filter (L). Depending on the length of the filter, the application to the timeseries the MA gets shifted, longer filters have a higher shift then shorter ones we visualize this behavior in \ref{fig:chap2.3.3}.
Many different indicators are built upon the Moving Average principle, mostly they are used in combinations of different lengths to create signals. In the following section, we introduce some of the most popular indicators based on the MA principle.

The actual challenge in using Moving average filters, is to figure out which length of the filter brings the most useful information.






#### 2.3.1. Equally-weighted Moving Average or SMA


&nbsp;

SMA stands for Simple Moving Average, depending on the length of the filter(L), L observations since the last noted observation will be considered. The observations are getting summarized and divided by the filterlength L. As stated in the name, all past observations are weighted equally. For every timestep, a new observation is considered and the last one eliminated [@SMA]. SMA's are very easily customized by changeing the length of the filter.


EqMA

\begin{equation}
  \label{eq:eqma}
  y_{t}=\frac{1}{L}\sum_{k=0}^{L-1}x_{t-k}
\end{equation}

- ${L}$ = filterlength
- ${x}$ = original series price e.g.





#####  2.3.1.1. Momentum

&nbsp;

Momentum is an indicator wether a market is bullish or bearish, it measures the "speed" of the trend direction in the market.
for a timespan $k$ the last price $p_k$, $k$ timesteps ago is subtracted from the last price. This is equivalent to applying and EqMA on the returns of a series. 


Momentum

\begin{equation}
  \label{eq:momentum}
  y_{t}=p_t - p_{t-K}
\end{equation}

- ${p_t}$ = prices of the series
- ${K}$   = Lag


```{r,chap2.3.1.1 , echo=FALSE,warning=FALSE, message=FALSE}
#head(GOOGL)
#googleclose=SMA( diff(Cl(GOOGL["2020-3::2020-4"])),10)
#chartSeries(googleclose,theme=chartTheme("white"))
#chartSeries(diff(GOOGL[,4],lag =10),subset="2020-3::2020-4",theme=chartTheme("white"),name= "Momentum")
```

\newpage

#### 2.3.2. Exponentially-weighted Moving Average

&nbsp;

Since not all observations are having the same influence on the future value, we can apply a weight to past observations. One method will be exponentially weighted Moving average. So we chose an optimal parameter to give past observations weights decreasing by $\alpha^{k}$. In comparison to the SMA [2.3.1.](#SMA) [@EMA] ,a EMA from the same length L, reacts faster than to price changes.

A skillful trader chooses an optimal $\alpha$ to increase the performance of the measurement. Weights could also be given individually by adding a weight vector to the filter.

EMA

\begin{equation}
  \label{eq:ema}
  y_{t}=\frac{1}{\sum_{k=0}^{m}\alpha^{k}}\sum_{k=0}^{m}\alpha^{k}x_{t-k}
\end{equation}

- ${m}$    = filterlength
- ${\alpha}$ = Parameter to weigh the observations

&nbsp;

#### 2.3.3. Moving Average Crossings {#macross-section}

&nbsp;

Moving average crossings are basically just different MA's with different lengths applied to a time-series. The points the filters then cross, will be used as a trading signal to go long, short or hold.
The "death" and "golden" cross are very popular trading patterns [@EMA]. If a shorter MA crosses the longer MA from above, its called a "golden cross" it is an indicator that the price will rise in the future and can be used to created the buy signal. In contrast stands the "death cross" vise versa, a shorter MA (popular L = 50) crosses a longer  MA(popular L = 200) from below, signalizing that further losses are in store.



Moving Average Crossing

\begin{equation}
  \label{eq:mac}
  y_{t}=\frac{1}{L_1}\sum_{k=0}^{L_1-1}x_{t-k} - \frac{1}{L_2}\sum_{k=0}^{L_2-1}x_{t-k}
\end{equation}

- ${L_1}$    = filterlength 1
- ${L_2}$    = filterlength 2
- ${0 < \alpha < 1}$  = Parameter to weigh the observations 

\newpage

An example of MA average crossings with 2 SMAs of different length is visualized in figure \ref{fig:chap2.3.3}. The prices are in \textcolor{green}{green} while the \textcolor{blue}{blue line} represents an \textcolor{blue}{50} day SMA an the \textcolor{red}{red line} a  \textcolor{red}{250 (1 year)} SMA. The crossing points of those two SMAs could now be used as trading signals. In this example these crossings wouldn't perform very well, therefore as mentioned earlier, finding the right length of the filter depends on each timeseries, their behavior and the preferences of the trader. 

&nbsp;

```{r, chap2.3.3, fig.cap="Moving Average Crossing", echo=FALSE,warning=FALSE, message=FALSE, fig.keep = 'last'}
par(mfrow=c(1,2))
n1=50
n2=250
chartSeries(GOOGL,type="line",subset="2015-3::2020-10",theme=chartTheme("white", bg.col="#FFFFFF"),name= "MA Crossings Google Nasdag")
addSMA(n=n1,on=1,col = "blue")
addSMA(n=n2,on=1,col = "red")

```



\newpage

### 2.4. Relative Strength Index

&nbsp;

The Relative strength index is a tool to measure momentum, the value indicates if positions are overvalued or undervalued.
the scale goes from 0 to 100. [@slide_trading_indicators_univariate].


\begin{equation}
  \label{eq:mac1}
  U_{t}=\begin{cases}1, \textnormal{if }    x_t \ge x_{t-1}\\0, \textnormal{otherwise} \end{cases}
  D_{t}=\begin{cases}1, \textnormal{if }  x_t < x_{t-1}\\0, \textnormal{otherwise} \end{cases}
\end{equation}

- ${X_t}$    =Original timeseries 

We then apply SMA or EMA of length N to $U_t$ and $U_t$ which converts them in $up_t(N)$ and $down_t(N)$
The RSI is now computet by:
\begin{equation}
  \label{eq:mac2}
 RSI_t(N)=100\frac{up_t(N)}{up_t(N)+down_t(N)}
\end{equation}

The original developer J. Welles Wilder Jr. proposed a length of N =14 in its work from 1978 [@RSI].
Traditional tradings signals based on the RSI  are the upper 70 or lower 30 limit to buy or sell.
Usually when the RSI is going over 70 it suppose that the asset is overvalued and in contrast when its under 30 then its undervalued.

&nbsp;


```{r,chap2.4 ,fig.cap="RSI", echo=FALSE, warning=FALSE, message=FALSE, fig.keep = 'last',fig.dim=c(7,4)}

chartSeries(GOOGL, subset="2020-01::2020-10",type="line",theme=chartTheme("white", bg.col="#FFFFFF"),name = "RSI EMA Google Nasdag",TA=NULL)
addRSI(n=14,maType="EMA")
```
\newpage

### 2.5. Moving Average Convergence Divergence

&nbsp;
The MACD is also a commonly used filter. The basic principle is to Subtract a longer EMA with length $L$ as in section   [2.3.3](#EMA) from a shorter EMA from length $S$  then smooth the result with another EMA with length $R$. As a result with can use the crossing of the 2 generated curves for trading. As an alternative we could use SMA's instead of EMA's

\begin{equation}
  \label{eq:MACD}
  \mathrm macd_t =\frac{1}{\sum_{k=0}^{t-1}\alpha^{k}} \sum_{k=0}^{t-1}\alpha^{k}x-{t-k}-\frac{1}{\sum_{k=0}^{t-1}\beta^{k}}\sum_{k=0}^{t-1}\beta^{k}x-{t-k}
\end{equation}


\begin{equation}
  \label{eq:signal MACD}
\mathrm  MACD signal_t=   \frac{1}{\sum_{k=0}^{t-1}\gamma^{k}}\sum_{k=0}^{t-1}\gamma^{k}x-{t-k}
\end{equation}



- $x_t$ = prices or log prices
- $S$   = length of the $short_1$ EMA           usually 12
- $L$   = length of the $long_1$  EMA           usually 26
- $R$   = length of the "double smoothing ema"  usually 9
- $\alpha$ = $1-\frac{1}{S}$, $\beta$  = $1-\frac{1}{L}$, $\gamma$ = $1-\frac{1}{R}$

$_1$ short and long in the meaning s<l, not buy sell

&nbsp;


```{r,chap2.5, fig.cap="MACD", echo=FALSE, warning=FALSE, message=FALSE, fig.keep = 'last',fig.dim=c(7,4)}
chartSeries(Cl(GOOGL),subset="2007-05::2009-01",theme=chartTheme("white", bg.col="#FFFFFF"),name= "MACD EMA GOOGL Closing")
addMACD(fast=12,slow=26,signal=9,type="EMA")
```
\newpage

### 2.6.  Bollinger bands
&nbsp;
Bollinger bands are a analysis tool founded by John Bollinger. It contains a moving average and an upper and lower band . The bands are defined by adding  a constant ${K}$ times a standard deviation ${\sigma_t}$ to the *Moving Average* for the upper , and subtracting it for the lower band.

\begin{equation}
  \label{eq:lower and upper bollingerbands}
  \mathrm{U_t}=MA_{t}+K\sigma, L_{t}=MA_{t}-K\sigma
\end{equation}

the variance from bollingers theory is calculated by:

\begin{equation}
  \label{eq:variance bollingerbands}
  \mathrm{\sigma_{t}^2}=\frac{1}{N}\sum_{k=0}^{N-1}(x_{t-k} -MA_t)^2
\end{equation}

The calculated ${\sigma_t}$ could be problematic because its derived from the original series and increases with the level, its non stationary. Therefore an other method to calculate the standard deviation could be used. As done in  section [2.2.2.](#garch-section) $\sigma_t$ could be provided by a GARCH, which would handle the increasing volatility.


- ${N}$ = usually the filterlength and the length considered for ${\sigma}$ are the same
- ${K}$ = Constant usually equals 2
- $\sigma_p$ = standard deviation of the series
- ${U_t}$ = upper band
- ${L_t}$ = lower band

&nbsp;

```{r,chap2.6 ,fig.cap="Bollinger Bands", echo=FALSE, warning=FALSE, message=FALSE, fig.keep = 'last',fig.dim=c(7,4)}
n=20
k=2
chartSeries(GOOGL,subset="2007-05::2009-01",theme=chartTheme("white", bg.col="#FFFFFF"),name= "bollinger bands")
addBBands(n=n,sd=k,maType = "SMA", draw = 'bands', on = -1)
#addTA(signal,type="S",col="red")
      
```
\newpage

### 2.7. Performance Indicators

#### 2.7.1. Sharpe Ratio

&nbsp;
 
Sharpe ratio is a very powerful and widely used ratio to measure performance. It describes return per risk.

\begin{equation}
  \label{eq:Sharperatio}
  \mathrm{SharpeRatio}=\frac{R_{p}-R_{f}}{\sigma}
\end{equation}

- ${R_p}$ = Return of Portfolio
- ${R_f}$ = Risk free Rate, mostly treasury bonds
- ${\sigma_p}$ = standard deviation of portfolios excess return (risk)

For comparing different series often the sharpe ratio is annualized with $\sqrt{250}$ and the ${R_f}$ is considered 0.


#### 2.7.2. Drawdown

Drawdown describes the maximum cumulated loss an investor is going to face if he isnt leaving a falling market at the turning point from rise to fall of an index. 

\begin{equation}
  \label{eq:Maximum Drawdown}
  \mathrm{MDD}=\frac{\textnormal{bottomvalue}-\textnormal{peakvalue}}{\textnormal{peakvalue}}
\end{equation}

Maximum drawdown is used as an indicator for risk or can be used as an optimization criterion[@Maxdrawdown].



### 2.8. Carry

&nbsp;

Carry trades are trading strategies were usually money is borrowed at a lower interest rate, than the investment is giving in return.
the risk of this strategy is based in the currency risk.




\newpage


