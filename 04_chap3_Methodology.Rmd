---
output: pdf_document
geometry: margin = 1in
---



```{r, echo=FALSE,message=F,warning=F}
#########################################attenzione !!!  this chunk is only to kniot this chapter can be deleted afterwards####
# load("data/data_mobi")
# source("add/libraries.R")
# ind <- data[,1:4]
# int <- data[,5:12]
```



## 3. Methodology

In this section, different models are created and compared with the buy and hold strategy.
You start with pure data analysis and then work your way from simple models to more and more complex ones.

### 3.1. Data Analysis

As mentioned in section [1.1](#ts-analysis) we are now going to analyze the data further to gain as much information as possible just by using some simple tools and comparisons.

#### 3.1.1. Correlation

&nbsp;

One could nearly tell just by looking at the indexes how strong they're correlated. The correlation matrix in table \ref{tab:cortable} confirms the assumption, the correlation is nearly 1 for every index to each other.

```{r cortable, echo=FALSE}
log_ind <-    log(ind)      #logarithm of the series
log_ret_ind <- na.exclude(diff(log_ind)) #log returns of the se
x<-as.data.frame(log_ind)
c <- as.data.frame(cor(x))  # showing corelations
kbl(c, caption="Correlations oft the four indexes", booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```


#### 3.1.2. transformation, volatility and clusters

&nbsp;

Applying the natural logarithm to the series is an approach to cancel out increasing volatility \ref{fig:chap3.1.2}. The strong upward drift is still visible, the original series has more than doubled to its original price over the whole timespan (index 4) \ref{fig:chap1.1}. 


```{r, chap3.1.2, echo = F, fig.dim=c(7,3.0), fig.cap="Visualization log indizes"}
par(mfrow=c(1,1))
plot(log(ind), main="Indexes 1-4", col=1:4, lwd=1, legend.loc="topleft") #plotting original data
```

\newpage

By taking the returns of the transformed series we can visualize volatility clusters as seen in figure \ref{fig:chap3.1.3}. The first value of the series is eliminated because of the differences,
Clearly visible are the high spikes in the times of the financial crisis 2007-2009. Also at the end of the series, the impact of COVID19 in march 2020 is remarkable.

```{r sharp_vola_table, echo=FALSE}
sd_vec <- apply(log_ret_ind, MARGIN = 2, FUN = sd)
sharpe_vec <- apply(log_ret_ind, MARGIN = 2, FUN = sharpe_fun)
sharp_vola_df <- data.frame(rbind(round(sd_vec, 4), round(sharpe_vec, 4)))
colnames(sharp_vola_df) <- c("Index 1", "Index 2", "Index 3", "Index 4")
rownames(sharp_vola_df) <- c("Volatility", "Sharpe-Ratio")

# sharp_vola_df$`Index 1` = cell_spec(sharp_vola_df$`Index 1`, color="black")
# sharp_vola_df$`Index 2` = cell_spec(sharp_vola_df$`Index 2`, color="red")
# sharp_vola_df$`Index 3` = cell_spec(sharp_vola_df$`Index 3`, color="green")
# sharp_vola_df$`Index 4` = cell_spec(sharp_vola_df$`Index 4`, color="blue")

kbl(sharp_vola_df, caption="Sharpe Ratio and Volatility of the 4 indexes (log-returns).", booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  column_spec(2, color="#000000") %>%
  column_spec(3, color="#DF536B") %>%
  column_spec(4, color="#61D04F") %>%
  column_spec(5, color="#2297E6")
```

The unconditional volatility of the indexes are \textcolor{black}{0.64e-06}, \textcolor{red}{4..1e-06}, \textcolor{green}{8.5e-06}, \textcolor{blue}{15.6e-06}. Which means the first index is 24 times more volatile than the fourth index.

&nbsp;

```{r, chap3.1.3, include=T, echo = F,message=F,  fig.cap="Log returns"}
plot(log_ret_ind, main="Log_returns indexes",legend.loc="topleft") #plot log returns
```

\newpage

##### 3.1.2.1. Autocorrelation of log returns

&nbsp;

By computing the ACF of the squared log-returns we see that the volatility cluster has very long dependency structures.


```{r,chap3.1.2.1,include=T, echo = F,message=F, fig.cap="ACF log returns",fig.dim=c(15,10)}
par(mfrow=c(2,2))
x1= log_ret_ind[,1]
x2= log_ret_ind[,2]
x3= log_ret_ind[,3]
x4= log_ret_ind[,4]

acf(abs(x1^2),lag.max = 50,main= "ACF INDEX 1 ,10-2003/03-2020" )
acf(abs(x2^2),lag.max = 50,main= "ACF INDEX 2 ,10-2003/03-2020" )
acf(abs(x3^2),lag.max = 50,main= "ACF INDEX 3 ,10-2003/03-2020" )
acf(abs(x4^2),lag.max = 50,main= "ACF INDEX 4 ,10-2003/03-2020" )

```

For further analysis, we concentrate on different timespans, because in most models it makes little sense to consider the whole time-series from 2003 till today.

\newpage

```{r,chap3.1.4, include=F, echo = F,message=F,fig.dim=c(15,15), fig.cap="Visualization log returns"}
par(mfrow=c(4,2))
start_date="2018-01-01"
in_sample ="2018-01-01"
x_ind<-log_ret_ind[paste(start_date,"/",sep="")]
vola_x_ind=format(round(diag(as.matrix(var(x_ind))),10),scientific= T)
y_ind<-log_ind[paste(start_date,"/",sep="")]
vola_y_ind=format(round(diag(as.matrix(var(y_ind))),10),scientific= T)
    
plot(x_ind[,1],main=paste("Log_returns index,","vola=",vola_x_ind[1],sep=" "),col = 1)
plot(y_ind[,1],main=paste("Log index 1,","vola=",vola_y_ind[1],sep=" "),col = 1)
plot(x_ind[,1],main=paste("Log_returns index,","vola=",vola_x_ind[2],sep=" "),col = 2)
plot(y_ind[,2],main=paste("Log index 2,","vola=",vola_y_ind[2],sep=" "),col = 2)
plot(x_ind[,1],main=paste("Log_returns index,","vola=",vola_x_ind[3],sep=" "),col = 3)
plot(y_ind[,3],main=paste("Log index 3,","vola=",vola_y_ind[3],sep=" "),col = 3)
plot(x_ind[,1],main=paste("Log_returns index,","vola=",vola_x_ind[4],sep=" "),col = 4)
plot(y_ind[,4],main=paste("Log index 4,","vola=",vola_y_ind[4],sep=" "),col = 4)
```

\newpage

### 3.2. Trading

In section 2 we have learned different indicators and models for time-series-analysis. Some of these models and indicators are now used to trade the indexes we've introduced in the previous section.

To do so we need to create trading signals based on the models and indicators. For example, we are using the MA Crossings, as mentioned [2.3.3.](#macross-section) the points where the two MAs cross, are now used to create a trading signal. when the longer MA comes from below to the crossing we are going long the asset and if it approaches the point from above we're shorting the position. Technically we apply a 1 to a vector at each crossing, where we intend to buy and apply a -1 at the points we want to sell.

#### 3.2.1. Buy and Hold Performance {#bnh-section}

&nbsp;

As mentioned earlier the goal of this work is trying to outperform the buy and hold strategy. Because the series all have a strong upward trend this task is very tricky. Buy and hold has very low trading costs because the underlying is just bought once. According to swissquote [@swissquote] costs for asset trades over 50k, are 190 USD per trade $_1$, so these costs should also be taken into consideration for choosing the strategy. In this work, we've excluded these costs and only concentrate on the trades themselves.

$_1$ notice: This fee is only for private investors, conditions may differ for institutions.

For the following models a good comparability should be achieved. To ensure this, all models are created with the same out-of-sample range, with the start date 2019-01-01.

##### 3.2.1.1. Portfolios

&nbsp;

By focussing on trading we build a portfolio for the 4 indexes. One approach would be the equally weighted portfolio with weights for every index of $\frac{1}{4}$. As we've seen in figure \ref{fig:chap3.1.3} The volatility of the indexes strongly differs, meaning that index 4 would have the most impact, nearly 50 %, on the portfolio by weighing it equally. To cancel this effect, it may be useful to size the position with the inverse volatility, so the indexes have the weights seen in the table on the right side.

\begin{equation}
  \label{eq:standardizing vola}
 procentual  share \sigma_k = \frac{\sigma_k}{\sum_{k=1}^{4}\sigma_k}
\end{equation}

```{r, chap3.2.1, echo=FALSE,include=T}
sigmas=diag(var(ind))
sigmas_1=1/sigmas
sigma_shares=as.matrix(100*sigmas/sum(sigmas))
sigma_shares_1=as.matrix(100*sigmas_1/sum(sigmas_1))
perc=cbind(sigma_shares,sigma_shares_1)
colnames(perc) <- c("percentage of vola when equally weighted","weights when weigthed with inverse vola")
perc=as.data.frame(perc)
kbl(perc, caption="shares", booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

\newpage

### 3.3. AR trading

As with the theory data set, these 4 indexes are non-stationary series. For the AR models, the stationary log-returns of the time series are used. First we look at index 1, the series with the lowest volatility but with the highest Sharpe ratio (highest return with lowest risk). To find the best possible AR model, different AR(*p*) models are fitted with different model orders *p* for different in-sample ranges. For each model the out-of-sample sharpe is calculated. The execution of this calculation leads to the solution shown in table \ref{tab:ar1table}.

```{r ar1table, echo=FALSE}
load("data/R_Files/optim_ar_1.RData")

part1 <- opt_ar_1[1:8,]
part2 <- opt_ar_1[9:16,]
part1["|"] <- rep("|", 8)

kbl(cbind(part1, part2), caption="Solution of the AR-Model calculation.", booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

With these calculations, the optimal model is an AR(2) with the start date 2018-01-01. Note that the Index 1 series has a very flat rising trend and a rising upward trend towards the end of 2018, so only a short in-sample is needed for this optimal model. It is interesting that the long in-sample ranges (2003 to 2007) also lead to relatively good performances. This probably has something to do with the fact that trend at the beginning of the series has a greater influence on the model than the middle part.

```{r, chap3.2.2.1, echo=FALSE, fig.cap="Visualization of the optimal AR(2)-Model.", fig.dim=c(11, 6), out.width = "90%"}
load("data/R_Files/optim_ar_1.RData")
load("data/R_Files/optim_ar_2.RData")
load("data/R_Files/optim_ar_3.RData")
load("data/R_Files/optim_ar_4.RData")

op2 <- c(as.character(opt_ar_1[which.max(opt_ar_1[,2]),][1]),
         as.character(opt_ar_2[which.max(opt_ar_2[,2]),][1]),
         as.character(opt_ar_3[which.max(opt_ar_3[,2]),][1]),
         as.character(opt_ar_4[which.max(opt_ar_4[,2]),][1]))

op3 <- c(as.numeric(opt_ar_1[which.max(opt_ar_1[,2]),][2]),
         as.numeric(opt_ar_2[which.max(opt_ar_2[,2]),][2]),
         as.numeric(opt_ar_3[which.max(opt_ar_3[,2]),][2]),
         as.numeric(opt_ar_4[which.max(opt_ar_4[,2]),][2]))

op4 <- c(as.numeric(opt_ar_1[which.max(opt_ar_1[,2]),][3]),
         as.numeric(opt_ar_2[which.max(opt_ar_2[,2]),][3]),
         as.numeric(opt_ar_3[which.max(opt_ar_3[,2]),][3]),
         as.numeric(opt_ar_4[which.max(opt_ar_4[,2]),][3]))

op1 <- round(apply(log_ret_ind["2019-01-01/"], MARGIN = 2, FUN = sharpe_fun), 3)

p1 <- perfplot_ar(optim_ar_obj=opt_ar_1, lr_series=log_ret_ind, inx=1, insamp="2019-01-01", plotter=FALSE, returner=TRUE)

trade_1 <- trading_counter(na.exclude(p1$signal))
return_bnh_1 <- round(tail(p1$perf_bnh, 1) * 1000000)
return_ar_1 <- round(tail(p1$perf_ar, 1) * 1000000)
return_ar_t_1 <- round((tail(p1$perf_ar, 1) * 1000000) - trade_1*190)

ar_plotter(perf_bnh=p1$perf_bnh, perf_ar=p1$perf_ar, start_date=op2[2], inx=1, p=op4[1], sharpe_bnh=op1[1], sharpe_ar=op3[1], signal=p1$signal)
```

The upper part of figure \ref{fig:chap3.2.2.1} shows the optimal model compared to buy and hold. The AR(2) has a higher sharpe than buy and hold. Regarding performance, it should be mentioned that the sharpe optimization of the algorithm is based on daily trades. The signals for a trade are newly generated with each daily forecast and could lead to high trading costs (positive signals = long, negative signals = short).

\newpage

The lower part of figure \ref{fig:chap3.2.2.1} shows the trading signals visually. If one were to trade exactly according to the AR(2) forecasts, one would make a trade at each vertical line, a total of `r trade_1` trades would have to be made during out-of-sample periods. If you want to include trading costs in the performance, an AR model with daily trades would have to be much better than this one to outperform buy-and-hold.

But what influence do the trading costs really have? If you assume a fictitious investment amount of 1 million USD and you have to pay 190 USD for each trade, as mentioned in section [3.2.1](#bnh-section). Invest this amount at the beginning of the out-of-sample period. With the buy-and-hold strategy, a return of USD `r return_bnh_1` is achieved by the end of the out-of-sample range. Without trading costs the AR(2) would yield USD `r return_ar_1`, with trading costs only USD `r return_ar_t_1`. To get at least the same return on the investment, the trading costs should not exceed USD 39. Note that in addition to the trading costs, the initial investment amount also matters.

Thus it can be said that for index 1, the buy-and-hold strategy cannot be outperformed by an AR(2) model. Nevertheless, one would like to apply this procedure also to the other indexes, possibly the generated models perform better for the other time-series.

```{r artable, echo=FALSE}
p2 <- perfplot_ar(optim_ar_obj=opt_ar_2, lr_series=log_ret_ind, inx=2, insamp="2019-01-01", plotter=FALSE)
p3 <- perfplot_ar(optim_ar_obj=opt_ar_3, lr_series=log_ret_ind, inx=3, insamp="2019-01-01", plotter=FALSE)
p4 <- perfplot_ar(optim_ar_obj=opt_ar_4, lr_series=log_ret_ind, inx=4, insamp="2019-01-01", plotter=FALSE)

trade_2 <- trading_counter(na.exclude(p2$signal))
trade_3 <- trading_counter(na.exclude(p3$signal))
trade_4 <- trading_counter(na.exclude(p4$signal))
trades <- c(trade_1, trade_2, trade_3, trade_4)

para <- data.frame(op2, op3, op4, op1, trades)
rownames(para) <- c("Index 1", "Index 2", "Index 3", "Index 4")
colnames(para) <- c("StartDate", "AR-Sharpe", "p", "B&H-Sharpe", "Trades")

kbl(para, caption="Optimal AR-Models.", booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```
Table \ref{tab:artable} shows the solutions of all 4 indexes. The algorithm finds an optimal AR model with model order 1 for all 3 remaining time-series. In contrast to index 1, it is noticeable that a much longer in-sample range is required for the optimal model. This also makes sense because the indexes show an increasing trend during the complete in-sample range. Index 1 is less volatile and has a less strong trend compared to the others, so a strong increase in the trend (spring 2018) has a greater effect on model performance. The other 3 series are more volatile, have larger peaks and therefore the model performance is less influenced by single events (like the increasing trend in spring 2018). The number of trades is much smaller for all models than for the first one. The out-of-sample performances are better than the corresponding buy-and-hold performances for all AR models.

Because of the trading costs, it has been seen with index 1 that the AR model, despite better performance, generates a lower return than buy-and-hold. But what about the other indexes. You choose the same investment amount and trading costs as before and you get the following table \ref{tab:artable2}.

```{r artable2, echo=FALSE}
return_bnh_2 <- round(tail(p2$perf_bnh, 1) * 1000000)
return_bnh_3 <- round(tail(p3$perf_bnh, 1) * 1000000)
return_bnh_4 <- round(tail(p4$perf_bnh, 1) * 1000000)
returns_bnh <- c(return_bnh_1, return_bnh_2, return_bnh_3, return_bnh_4)

return_ar_2 <- round(tail(p2$perf_ar, 1) * 1000000)
return_ar_3 <- round(tail(p3$perf_ar, 1) * 1000000)
return_ar_4 <- round(tail(p4$perf_ar, 1) * 1000000)
returns_ar <- c(return_ar_1, return_ar_2, return_ar_3, return_ar_4)

return_ar_t_2 <- round((tail(p2$perf_ar, 1) * 1000000) - trade_2*190)
return_ar_t_3 <- round((tail(p3$perf_ar, 1) * 1000000) - trade_3*190)
return_ar_t_4 <- round((tail(p4$perf_ar, 1) * 1000000) - trade_4*190)
returns_at_t <- c(return_ar_t_1, return_ar_t_2, return_ar_t_3, return_ar_t_4)


para <- data.frame(returns_bnh, returns_ar, returns_at_t)
rownames(para) <- c("Index 1", "Index 2", "Index 3", "Index 4")
colnames(para) <- c("Buy & Hold", "AR without tradingcost", "AR with tradingcost")

kbl(para, caption="Out-of-Sample Returns.", booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

Despite the trading costs, daily trading with the indexes 2-4 generates a higher return than the passive buy and hold strategy.

\newpage

Figure \ref{fig:chap3.2.2.2} shows the out-of-sample performance of the second index. Right at the beginning of the series, the AR model signals a trade (one goes short and immediately long again). This trade immediately leads to an increase in performance which already outperforms the passive strategy. Further signaled trades will only occur again later. During the Corona crisis (spring 2020), one can see several close consecutive trades. The volatility increases sharply during this period, the AR model catches this event well and remains positive.

```{r, chap3.2.2.2, echo=FALSE, fig.cap="Visualization of the optimal Index 2 AR(1)-Model.", fig.dim=c(11, 6), out.width = "85%"}
ar_plotter(perf_bnh=p2$perf_bnh, perf_ar=p2$perf_ar, start_date=op2[2], inx=2, p=op4[2], sharpe_bnh=op1[2], sharpe_ar=op3[2], signal=p2$signal)
```
The optimal AR model for the third index shows almost the same behavior as index 2 (as seen in figure \ref{fig:chap3.2.2.3}), the only difference being an additional trade in spring 2019.

```{r, chap3.2.2.3, echo=FALSE, fig.cap="Visualization of the optimal Index 3 AR(1)-Model..", fig.dim=c(11, 6), out.width = "85%"}
ar_plotter(perf_bnh=p3$perf_bnh, perf_ar=p3$perf_ar, start_date=op2[3], inx=3, p=op4[3], sharpe_bnh=op1[3], sharpe_ar=op3[3], signal=p3$signal)
```

\newpage

The fourth index, which is shown in the lower figure \ref{fig:chap3.2.2.3}, is interesting again. The optimal AR model predicts almost the same behavior as buy-and-hold for almost the entire duration. Only during the highly volatile phase of the Corona crisis are the same trades listed as for index 2 and 3.

```{r, chap3.2.2.4, echo=FALSE, fig.cap="Visualization of the optimal Index 4 AR(1)-Model.", fig.dim=c(11, 6), out.width = "85%"}
ar_plotter(perf_bnh=p4$perf_bnh, perf_ar=p4$perf_ar, start_date=op2[4], inx=4, p=op4[4], sharpe_bnh=op1[4], sharpe_ar=op3[4], signal=p4$signal)
```
So it can be said that with very simple autoregressive models, a tool has been found with which it is possible to outperform the popular buy-and-hold strategy. Consider that this is only the case in this example with defined conditions. Depending on how high the trading costs and the amount of investment are, the returns can vary greatly. In addition to the buy-and-hold strategy, where you only buy and hold, you have to trade effectively with an active trading strategy.

\newpage

### 3.4. Single Simple Moving average trading

SMA undso

```{r, chap3.2.3.1, echo=FALSE, fig.cap="Placeholder", fig.dim=c(11, 6), out.width = "100%"}
optim_sma1 <- optimize_sma(x=log_ret_ind, inx=1)
```

\newpage

```{r, chap3.2.3.2, echo=FALSE, fig.cap="Placeholder", fig.dim=c(11, 6), out.width = "100%"}
optim_sma2 <- optimize_sma(x=log_ret_ind, inx=2)
```

\newpage

```{r, chap3.2.3.3, echo=FALSE, fig.cap="Placeholder", fig.dim=c(11, 6), out.width = "100%"}
optim_sma3 <- optimize_sma(x=log_ret_ind, inx=3)
```

\newpage

```{r, chap3.2.3.4, echo=FALSE, fig.cap="Placeholder", fig.dim=c(11, 6), out.width = "100%"}
optim_sma4 <- optimize_sma(x=log_ret_ind, inx=4)
```

\newpage

### 3.5. Moving average crossings trading
 
As a third approach we apply 2 SMAs on every time series and trade them with the common Ma crossings rules introduced in [2.3.3.](#macross-section).
With an optimaztion we're trying to find the best filterlengths for the 2 SMA's.

#### 3.5.1. Optimization

&nbsp;

For the Optimization the same procedure as in the previous sections gets executed.

The optimization has two levels, the "first level" or outer level is  narrowing the insample timespan from (2003-01 / 01-2019) to (01-2018 / 01-2019). In the inner level "second level" the real optimization is calculated by finding the optimal combination of filters, regarding maximum sharpe and maximum drowndown trading them with the common crossing trading rules.

Hyperparameter:

- $\textnormal{date}$ = startdate for the insample timespan $\textnormal{2003-01-01} < \textnormal{date} \le \textnormal{2018-01-01}\textnormal{, by 1 trading year}$

Parameters$_2$:

- $L1$ = length of shorter SMA  with $1 < L1 \le  50$ 
- $L2$ = length of longer  SMA with  $100 < L1 \le 250$

Figure \ref{fig:chap3.2.4.1.1} visualizes one iteration of the optimization. For the fixed L1 =1, L2 is variable and the optimal sharpe is calculated. This procedure is done for every filterlenght as described in $_2$ and further visualized in \ref{fig:chap3.2.4.1.2}. The drawdown is also calculated as seen in plot \ref{fig:chap3.2.4.1.3}. Notice that the visualization is from the starting date 2015.


```{r, echo=FALSE, fig.cap="Hyperoptimization", include=F,}
load("data/R_Files/optim_ma_cross_obj_1_xts.RData")###loadind xts datasets index1 
load("data/R_Files/optim_ma_cross_obj_1.RData")    #loading perf mat index1
``` 

```{r, chap3.2.4.1.1, echo=FALSE, fig.cap="optimization sharpe L1=1 ,L2 = variable", include=T,out.width = "100%",fig.dim=c(16, 7.4)}
sharpfirst=as.array((ind1_opt$perf_mat)[,2])
plot(sharpfirst[1:151],type="l",ylab="Max sharpe",xlab="L1 fixed=1, L2 variable on x-axis ",main="filter optimization start 2015  ",col="black")
```  
\newpage
```{r, chap3.2.4.1.2, echo=FALSE, fig.cap="sharpe index 1", include=T,out.width = "100%",fig.dim=c(16, 7.4)}
plot((ind1_opt$perf_mat)[,2],type="l",ylab="Max Sharpe",xlab="",main="filter optimization start: 2015 ")
abline(h=1.12, col = "red")

```
&nbsp;

```{r, chap3.2.4.1.3, echo=FALSE, fig.cap="drawdown index 1", include=T,out.width = "100%",fig.dim=c(16, 7.4)}
plot((ind1_opt$perf_mat)[,3],type="l",ylab="Max Drawdown",xlab="",main="filter optimization start: 2015 ",col="green")
abline(h=-0.0069, col = "red")
```
The pattern we observe in \ref{fig:chap3.2.4.1.2} is the repeating optimization by fixing L1 and variabling L2. The maximum sharpe was found with L1= *1*, L2= *106*, sharpe =  *1.225*. When we look at the red line we can clearly see there are other similar  high spikes in other filter lengths. The green pattern in \ref{fig:chap3.2.4.1.3} shows interesting news. at L1 = 17,18, 19 and 20 we see a cluster of high spikes, all pretty similar. The optimum in this plot is by L1 = *18* and L2 = *102* and a drawdown 6.55e-3, laying in the described cluster . But when we look at the red line drawn, we can clearly see there's another high spike at the beginning of the plot. This spike is identical with the maximum sharpe in \ref{fig:chap3.2.4.1.2}. So these result could just be random and the solution is not finite $_3$.

\newpage

```{r, chap3.2.4.1.4, echo=FALSE, fig.cap="Hyperoptimization Index 1", include=T,fig.keep = 'last',fig.dim=c(16, 7.1),out.width = "100%"}
##################sharpe maxdrow
plot(optim_ma_cross_obj_1_xts[,1],ylim= c(-2,4),main= "Max Sharpe/Drawdown Hyperoptimization Index 1",col="black")
lines(optim_ma_cross_obj_1_xts[,4]*100,lwd=2,col="green")
addLegend("topleft", on=1, legend.names = c("Sharpe ", "MaxDrawdown * 100 "), lty=c(1, 1), lwd=c(2, 2),col=c("black","green"))
events <- xts("Figure 20/21/22", as.Date("2015-01-01"))
addEventLines(events,srt=90,pos=2,lty=2,)
```
&nbsp;

```{r, chap3.2.4.1.5, echo=FALSE, fig.cap="Hyperoptimization Index 1", include=T,fig.keep = 'last',fig.dim=c(16, 7.1),out.width = "100%"}
################## lengths
plot(optim_ma_cross_obj_1_xts[,2:3],type="l",col= "red",main= "Filterlengths Hyperoptimization Index 1 ",)
lines(optim_ma_cross_obj_1_xts[,5:6], col=c("blue"),lwd=2)
addLegend("topleft", on=1, legend.names = c("Sharpe long filter", "Drawdown long filter"), lty=c(1, 1), lwd=c(1, 1),col=c("red","blue"))
addLegend("bottomleft", on=1, legend.names = c("Sharpe short filter", "Drawdown short filter"), lty=c(1, 1), lwd=c(1, 1),col=c("red","blue"))
addEventLines(events,srt=90,pos=2,lty=2)

```

By plotting the sharperatio / max drawdown in \ref{fig:chap3.2.4.1.4} and the optimal filterlenghts L1/L2 in \ref{fig:chap3.2.4.1.5}, for every timesstep, the  Hyperoptimization is visualized. 
In figure \ref{fig:chap3.2.4.1.4} index 1, one can observe a strong upward trend in both sharpe and maxdrawdown $_1$ , narrowing the insample timespan at the end of the plot. In plot \ref{fig:chap3.2.4.1.1} we observe at the end , from 2017 to 2018, nearly the same lengths of the filters in both sharpe and drawdown. Overall the plots do not look the same until 2016 where they get very close. For further analysis we exclude the insample start 2016 2017 and 2018 



Because of the problem we've mentioned in $_3$ the best result could also be just random. 
To solve this problem one could do the optimization again, indivdually analyze every in-sample timespan or plot the 5 best filterlenghts for every timestep.

$_1$ Note: maxdrawdown is scaled by the factor 100 to visualize it in the same plot 
\newpage
As noticed the Visualizations are only done for index one, We've done the same optimization for all indexes the result are shown in 



\newpage
&nbsp;

```{r, chap3.2.2, echo=FALSE, fig.cap="conversion data", include=T, fig.keep = 'last'}
n1=15
n2 =135
mobidat= data[,4] # chose the row and horizon
start="2019-01-01"
end = "2020-04-31"
horizon=paste(start,"::",end,sep = "")
sma1 <-SMA(mobidat,n=n1)
sma2 <-SMA(mobidat,n=n2)
signal <-rep(0,length(sma1))
signal[which(sma1>sma2&lag(sma1)<lag(sma2))]<-1
signal[which(sma1<sma2&lag(sma1)>lag(sma2))]<--1
signal[which(sma1>sma2)]<-1
signal[which(sma1<sma2)]<--1
signal=reclass(signal,sma1)
chartSeries(mobidat,subset=horizon,theme=chartTheme("white", bg.col="#FFFFFF"),name= "sMa",type="")
addSMA(n=n1,on=1,col = "blue")
addSMA(n=n2,on=1,col = "red")
addTA(signal,type="S",col="red")
trade   =   Lag(signal[horizon],1)
return  =   diff(log(mobidat))
ret = return*trade
names(ret)="filter"
```

The intuition says that this strategy may not be so bad , why leaving a trend market instead of buy and hold?

Keeping in mind that the filterlengths L need the same 
initializationtime as the length of filter  itself. For example when the optimization
\newpage

```{r, chap3.2.3, echo=FALSE, fig.cap="conversion data",include=T,fig.keep = 'last'}
#SharpeRatio(ret,FUN="StdDev")
#chart.Bar(ret,main="returns daily")
#chart.CumReturns(ret, main="Naive Rule: Cum Returns")
#chart.Drawdown(ret,main="Naive Rule: Percentage Drawdown")
charts.PerformanceSummary(ret, main="Naive Buy Rule")
#SharpeRatio(data,FUN="StdDev")
#ames(ret)="filter"
```