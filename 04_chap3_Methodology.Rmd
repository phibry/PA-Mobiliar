---
output: pdf_document
geometry: margin = 1in
---



```{r, echo=FALSE,message=F,warning=F}
#########################################attenzione !!!  this chunk is only to kniot this chapter can be deleted afterwards####
# load("data/data_mobi")
# source("add/libraries.R")
# ind <- data[,1:4]
# int <- data[,5:12]
```



## 3. Methodology

In this section, different models are created and compared with the buy-and-hold strategy.
You start with pure data analysis and then work your way from simple models to more and more complex ones.



### 3.1. Data Analysis {#ts-analysis-depth}


As mentioned in section [1.1](#ts-analysis) we are now going to analyze the data further to gain as much information as possible just by using some simple tools and comparisons.

#### 3.1.1. Correlation

&nbsp;

One could nearly tell just by looking at the indexes how strong they're correlated. The correlation matrix in table \ref{tab:cortable} confirms the assumption, the correlation is nearly 1 for every index to each other.

```{r cortable, echo=FALSE}

log_ind <-    log(ind)      #logarithm of the series
log_ret_ind <- na.exclude(diff(log_ind)) #log-returns of the se
x<-as.data.frame(log_ind)
c <- as.data.frame(cor(x))  # showing corelations
kbl(c, caption="Correlations oft the four indexes.", booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```


#### 3.1.2. Transformation, Volatility and Clusters {#vola-sharpe}

&nbsp;

Applying the natural logarithm to the series is an approach to cancel out increasing volatility \ref{fig:chap3.1.2}. The strong upward drift is still visible, the original series has more than doubled to its original price over the whole timespan (index 4) \ref{fig:chap1.1}. 


```{r, chap3.1.2, echo = F, fig.dim=c(7,3.0), fig.cap="Visualization log indizes."}
par(mfrow=c(1,1))
plot(log(ind), main="Indexes 1-4", col=1:4, lwd=1, legend.loc="topleft") #plotting original data
```

\newpage

By taking the returns of the transformed series we can visualize volatility clusters as seen in figure \ref{fig:chap3.1.3}. The first value of the series is eliminated because of the differences. Clearly visible are the high spikes in the times of the financial crisis 2007-2009. Also at the end of the series, the impact of Covid19 in march 2020 is remarkable.

```{r sharp_vola_table, echo=FALSE}
sd_vec <- apply(log_ret_ind, MARGIN = 2, FUN = sd)
sharpe_vec <- apply(log_ret_ind, MARGIN = 2, FUN = sharpe_fun)
sharp_vola_df <- data.frame(rbind(round(sd_vec, 4), round(sharpe_vec, 4)))
colnames(sharp_vola_df) <- c("Index 1", "Index 2", "Index 3", "Index 4")
rownames(sharp_vola_df) <- c("Volatility", "Sharpe-Ratio")

kbl(sharp_vola_df, caption="Sharpe Ratio and Volatility of the 4 indexes (log-returns).", booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  column_spec(2, color="#000000") %>%
  column_spec(3, color="#DF536B") %>%
  column_spec(4, color="#61D04F") %>%
  column_spec(5, color="#2297E6")
```

As we can see in the table the volatility of index 4 is as much as  5 times higher than the volatility of index 1 and the Sharpe ratio behaves inverse to the risk.

&nbsp;

```{r, chap3.1.3, include=T, echo = F,message=F,  fig.cap="Log-Returns."}
plot(log_ret_ind, main="Log_returns indexes",legend.loc="topleft") #plot log-returns
```

\newpage

##### 3.1.2.1. Autocorrelation of log-returns

&nbsp;

By computing the ACF of the squared log-returns we see that the volatility cluster have very long dependency structures.


```{r,chap3.1.2.1,include=T, echo = F,message=F, fig.cap="ACF log-returns.", fig.dim=c(15,10)}
par(mfrow=c(2,2))
x1= log_ret_ind[,1]
x2= log_ret_ind[,2]
x3= log_ret_ind[,3]
x4= log_ret_ind[,4]

acf(abs(x1^2),lag.max = 50,main= "ACF INDEX 1 ,10-2003/03-2020" )
acf(abs(x2^2),lag.max = 50,main= "ACF INDEX 2 ,10-2003/03-2020" )
acf(abs(x3^2),lag.max = 50,main= "ACF INDEX 3 ,10-2003/03-2020" )
acf(abs(x4^2),lag.max = 50,main= "ACF INDEX 4 ,10-2003/03-2020" )

# chart.ACF(abs(x1^2),main= "ACF INDEX 1 ,10-2003/03-2020")
# chart.ACF(abs(x2^2),main= "ACF INDEX 2 ,10-2003/03-2020")
# chart.ACF(abs(x3^2),main= "ACF INDEX 3 ,10-2003/03-2020")
# chart.ACF(abs(x4^2),main= "ACF INDEX 4 ,10-2003/03-2020")

```

\newpage

```{r,chap3.1.4, include=F, echo = F,message=F,fig.dim=c(15,15), fig.cap="Visualization log-returns."}
par(mfrow=c(4,2))
start_date="2018-01-01"
in_sample ="2018-01-01"
x_ind<-log_ret_ind[paste(start_date,"/",sep="")]
vola_x_ind=format(round(diag(as.matrix(var(x_ind))),10),scientific= T)
y_ind<-log_ind[paste(start_date,"/",sep="")]
vola_y_ind=format(round(diag(as.matrix(var(y_ind))),10),scientific= T)
    
plot(x_ind[,1],main=paste("Log_returns index,","vola=",vola_x_ind[1],sep=" "),col = 1)
plot(y_ind[,1],main=paste("Log index 1,","vola=",vola_y_ind[1],sep=" "),col = 1)
plot(x_ind[,1],main=paste("Log_returns index,","vola=",vola_x_ind[2],sep=" "),col = 2)
plot(y_ind[,2],main=paste("Log index 2,","vola=",vola_y_ind[2],sep=" "),col = 2)
plot(x_ind[,1],main=paste("Log_returns index,","vola=",vola_x_ind[3],sep=" "),col = 3)
plot(y_ind[,3],main=paste("Log index 3,","vola=",vola_y_ind[3],sep=" "),col = 3)
plot(x_ind[,1],main=paste("Log_returns index,","vola=",vola_x_ind[4],sep=" "),col = 4)
plot(y_ind[,4],main=paste("Log index 4,","vola=",vola_y_ind[4],sep=" "),col = 4)
```

\newpage

### 3.2. Trading

In section [2](#theory) we have learned different indicators and models for time-series-analysis. Some of these models and indicators are now used to trade the indexes we have introduced in the previous section.

To do so we need to create trading signals based on the models and indicators. For example, we are using the MA Crossings, as mentioned in section [2.3.3](#macross-section) the points where the two MAs cross, are now used to create a trading signal. when the longer MA comes from below to the crossing we are going long the asset and if it approaches the point from above we are shorting the position. Technically we apply a 1 to a vector at each crossing, where we intend to buy and apply a -1 at the points we want to sell.

#### 3.2.1. Buy-and-Hold Performance {#bnh-section}

&nbsp;

As mentioned earlier the goal of this work is trying to outperform the buy-and-hold strategy. Because the series all have a strong upward trend, this task is very tricky. Buy-and-hold has very low trading costs because the underlying is just bought once. According to swissquote [@swissquote] costs for asset trades over 50k, are 190 USD per trade $_1$, so these costs should also be taken into consideration for choosing the strategy. In this work, we have mostly excluded these costs and only concentrate on the trades themselves.

$_1$ notice: This fee is only for private investors, conditions may differ for institutions.

For the following models a good comparability should be achieved. To ensure this, all models are created with the same out-of-sample range, with the start date 2019-01-01.

##### 3.2.1.1. Portfolios

&nbsp;

By focussing on trading one could build a portfolio for the 4 indexes. One approach would be the equally weighted portfolio with weights for every index of $\frac{1}{4}$. As we have seen in figure \ref{fig:chap3.1.3} the volatility of the indexes strongly differs, meaning that index 4 would have the most impact, nearly 50 %, on the portfolio by weighing it equally. To cancel this effect, it may be useful to size the position with the inverse volatility, so the indexes have the weights seen in the table on the right side.

\begin{equation}
  \label{eq:standardizing vola}
 \textnormal{procentual}\:\textnormal{share}\:\sigma_k = \frac{\sigma_k}{\sum_{k=1}^{4}\sigma_k}
\end{equation}

```{r, chap3.2.1, echo=FALSE,include=T}
sigmas=diag(var(ind))
sigmas_1=1/sigmas
sigma_shares=as.matrix(100*sigmas/sum(sigmas))
sigma_shares_1=as.matrix(100*sigmas_1/sum(sigmas_1))
perc=cbind(sigma_shares,sigma_shares_1)
colnames(perc) <- c("Weight 1","Weight 2")
perc=as.data.frame(perc)
kbl(perc, caption="Shares.", booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  footnote(number=c("Weight 1: Equally weighted volatility in %", "Weight 2: Weighted with inverse volatility"))
```

\newpage

### 3.3. AR trading

As with the theory data set, these 4 indexes are non-stationary series. For the AR models, the stationary log-returns of the time-series are used. First we look at index 1, the series with the lowest volatility but with the highest sharpe ratio (highest return with lowest risk). To find the best possible AR model, different AR(*p*) models are fitted with different model orders *p* for different in-sample ranges. The parameters *p* and start date are so called hyper parameters, parameters which are set before modelling. For each model the out-of-sample sharpe is calculated. The execution of this calculation leads to the solution shown in table \ref{tab:ar1table}.

```{r ar1table, echo=FALSE}
part1 <- opt_ar_1[1:8,]
part2 <- opt_ar_1[9:16,]
part1["|"] <- rep("|", 8)

kbl(cbind(part1, part2), caption="Solution of the AR-Model calculation.", booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

With these calculations, the optimal model is an AR(2) with the start date 2018-01-01. Note that the index 1 series has a very flat rising trend and a rising upward trend towards the end of 2018, so only a short in-sample is needed for this optimal model. It is interesting that the long in-sample ranges (2003 to 2007) also lead to relatively good performances. This probably has something to do with the fact that the trend at the beginning of the series has a higher influence on the models than the middle part.

```{r, chap3.2.2.1, echo=FALSE, fig.cap="Visualization of the optimal AR(2)-Model.", fig.dim=c(11, 6), out.width = "90%"}
op2 <- c(as.character(opt_ar_1[which.max(opt_ar_1[,2]),][1]),
         as.character(opt_ar_2[which.max(opt_ar_2[,2]),][1]),
         as.character(opt_ar_3[which.max(opt_ar_3[,2]),][1]),
         as.character(opt_ar_4[which.max(opt_ar_4[,2]),][1]))

op3 <- c(as.numeric(opt_ar_1[which.max(opt_ar_1[,2]),][2]),
         as.numeric(opt_ar_2[which.max(opt_ar_2[,2]),][2]),
         as.numeric(opt_ar_3[which.max(opt_ar_3[,2]),][2]),
         as.numeric(opt_ar_4[which.max(opt_ar_4[,2]),][2]))

op4 <- c(as.numeric(opt_ar_1[which.max(opt_ar_1[,2]),][3]),
         as.numeric(opt_ar_2[which.max(opt_ar_2[,2]),][3]),
         as.numeric(opt_ar_3[which.max(opt_ar_3[,2]),][3]),
         as.numeric(opt_ar_4[which.max(opt_ar_4[,2]),][3]))

op1 <- round(apply(log_ret_ind["2019-01-01/"], MARGIN = 2, FUN = sharpe_fun), 3)

p1 <- perfplot_ar(optim_ar_obj=opt_ar_1, lr_series=log_ret_ind, inx=1, insamp="2019-01-01", plotter=FALSE, returner=TRUE)

trade_1 <- trading_counter(na.exclude(p1$signal))
return_bnh_1 <- round(tail(p1$perf_bnh, 1) * 1000000)
return_ar_1 <- round(tail(p1$perf_ar, 1) * 1000000)
return_ar_t_1 <- round((tail(p1$perf_ar, 1) * 1000000) - trade_1*190)

ar_plotter(perf_bnh=p1$perf_bnh, perf_ar=p1$perf_ar, start_date=op2[2], inx=1, p=op4[1], sharpe_bnh=op1[1], sharpe_ar=op3[1], signal=p1$signal)
```

The upper part of figure \ref{fig:chap3.2.2.1} shows the optimal model compared to buy-and-hold. The AR(2) has a higher sharpe than buy-and-hold. Regarding performance, it should be mentioned that the sharpe optimization of the algorithm is based on daily trades. The signals for a trade are newly generated with each daily forecast and could lead to high trading costs (positive signals = long, negative signals = short).

\newpage

The lower part of figure \ref{fig:chap3.2.2.1} shows the trading signals visually. If one were to trade exactly according to the AR(2) forecasts, one would make a trade at each vertical line, a total of `r trade_1` trades would have to be made during the out-of-sample period. If you want to include trading costs in the performance, an AR model with daily trades would have to be much better than this one to outperform buy-and-hold.

But what influence do the trading costs really have? If you assume a fictitious investment amount of 1 million USD and you have to pay 190 USD for each trade, as mentioned in section [3.2.1](#bnh-section). Invest this amount at the beginning of the out-of-sample period. With the buy-and-hold strategy, a return of USD `r return_bnh_1` is achieved by the end of the out-of-sample range. Without trading costs the AR(2) would yield USD `r return_ar_1`, with trading costs only USD `r return_ar_t_1`. To get at least the same return on the investment, the trading costs should not exceed USD 39. Note that all the parameters have to be taken in consideration for real life trading such as: trading costs, realizable trading volume, restrictions at the exchange, governmental restrictions, so on and so forth.

Thus it can be said that for index 1, the buy-and-hold strategy cannot be outperformed by an AR(2) model if trading costs are taken into account. Nevertheless, one would like to apply this procedure also to the other indexes, possibly the generated models perform better for the other time-series.

```{r artable, echo=FALSE}
p2 <- perfplot_ar(optim_ar_obj=opt_ar_2, lr_series=log_ret_ind, inx=2, insamp="2019-01-01", plotter=FALSE)
p3 <- perfplot_ar(optim_ar_obj=opt_ar_3, lr_series=log_ret_ind, inx=3, insamp="2019-01-01", plotter=FALSE)
p4 <- perfplot_ar(optim_ar_obj=opt_ar_4, lr_series=log_ret_ind, inx=4, insamp="2019-01-01", plotter=FALSE)

trade_2 <- trading_counter(na.exclude(p2$signal))
trade_3 <- trading_counter(na.exclude(p3$signal))
trade_4 <- trading_counter(na.exclude(p4$signal))
trades <- c(trade_1, trade_2, trade_3, trade_4)

para <- data.frame(op2, op3, op4, op1, trades)
rownames(para) <- c("Index 1", "Index 2", "Index 3", "Index 4")
colnames(para) <- c("StartDate", "AR-Sharpe", "p", "B&H-Sharpe", "Trades")

kbl(para, caption="Optimal AR-Models.", booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```
Table \ref{tab:artable} shows the solutions of all 4 indexes. The algorithm finds an optimal AR model with model order 1 for all 3 remaining time-series. In contrast to index 1, it is noticeable that a much longer in-sample range is required for the optimal model. This also makes sense because the indexes show an increasing trend during the complete in-sample range. Index 1 is less volatile and has a less strong trend compared to the others, so a strong increase in the trend (spring 2018) has a greater effect on model performance. The other 3 series are more volatile, have larger peaks and therefore the model performance is less influenced by single events (like the increasing trend in spring 2018). The number of trades is much smaller for all models than for the first one. The out-of-sample performances are better than the corresponding buy-and-hold performances for all AR models.

Because of the trading costs, it has been seen with index 1 that the AR model, despite the better performance, generates a lower return than buy-and-hold. But what about the other indexes. You choose the same investment amount and trading costs as before and you get the following table \ref{tab:artable2}.

```{r artable2, echo=FALSE}
return_bnh_2 <- round(tail(p2$perf_bnh, 1) * 1000000)
return_bnh_3 <- round(tail(p3$perf_bnh, 1) * 1000000)
return_bnh_4 <- round(tail(p4$perf_bnh, 1) * 1000000)
returns_bnh <- c(return_bnh_1, return_bnh_2, return_bnh_3, return_bnh_4)

return_ar_2 <- round(tail(p2$perf_ar, 1) * 1000000)
return_ar_3 <- round(tail(p3$perf_ar, 1) * 1000000)
return_ar_4 <- round(tail(p4$perf_ar, 1) * 1000000)
returns_ar <- c(return_ar_1, return_ar_2, return_ar_3, return_ar_4)

return_ar_t_2 <- round((tail(p2$perf_ar, 1) * 1000000) - trade_2*190)
return_ar_t_3 <- round((tail(p3$perf_ar, 1) * 1000000) - trade_3*190)
return_ar_t_4 <- round((tail(p4$perf_ar, 1) * 1000000) - trade_4*190)
returns_at_t <- c(return_ar_t_1, return_ar_t_2, return_ar_t_3, return_ar_t_4)


para <- data.frame(returns_bnh, returns_ar, returns_at_t)
rownames(para) <- c("Index 1", "Index 2", "Index 3", "Index 4")
colnames(para) <- c("Buy & Hold", "AR without tradingcost", "AR with tradingcost")

kbl(para, caption="Out-of-Sample Returns.", booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

Despite the trading costs, daily trading with the indexes 2-4 generates a higher return than the passive buy-and-hold strategy.

\newpage

Figure \ref{fig:chap3.2.2.2} shows the out-of-sample performance of the second index. Right at the beginning of the series, the AR model signals a trade (one goes short and immediately long again). This trade immediately leads to an increase in performance which already outperforms the passive strategy. Further signaled trades will only occur again later. During the Covid19 crisis (spring 2020), one can see several close consecutive trades. The volatility increases sharply during this period, the AR model catches this event well and remains positive.

```{r, chap3.2.2.2, echo=FALSE, fig.cap="Visualization of the optimal Index 2 AR(1)-Model.", fig.dim=c(11, 6), out.width = "85%"}
ar_plotter(perf_bnh=p2$perf_bnh, perf_ar=p2$perf_ar, start_date=op2[2], inx=2, p=op4[2], sharpe_bnh=op1[2], sharpe_ar=op3[2], signal=p2$signal)
```
The optimal AR model for the third index shows almost the same behavior as index 2 (as seen in figure \ref{fig:chap3.2.2.3}), the only difference being an additional trade in spring 2019.

```{r, chap3.2.2.3, echo=FALSE, fig.cap="Visualization of the optimal Index 3 AR(1)-Model.", fig.dim=c(11, 6), out.width = "85%"}
ar_plotter(perf_bnh=p3$perf_bnh, perf_ar=p3$perf_ar, start_date=op2[3], inx=3, p=op4[3], sharpe_bnh=op1[3], sharpe_ar=op3[3], signal=p3$signal)
```

\newpage

The fourth index, which is shown in the lower figure \ref{fig:chap3.2.2.3}, is interesting again. The optimal AR model predicts almost the same behavior as buy-and-hold for almost the entire duration. Only during the highly volatile phase of the Covid19 crisis are the same trades listed as for index 2 and 3.

```{r, chap3.2.2.4, echo=FALSE, fig.cap="Visualization of the optimal Index 4 AR(1)-Model.", fig.dim=c(11, 6), out.width = "85%"}
ar_plotter(perf_bnh=p4$perf_bnh, perf_ar=p4$perf_ar, start_date=op2[4], inx=4, p=op4[4], sharpe_bnh=op1[4], sharpe_ar=op3[4], signal=p4$signal)
```
So it can be said that with very simple autoregressive models, a tool has been found with which it is possible to outperform the popular buy-and-hold strategy. Consider that this is only the case in this example with defined conditions. Depending on how high the trading costs and the amount of investment are, the returns can vary greatly. In addition to the buy-and-hold strategy, where you only buy the financial instrument and then hold it, you have to trade effectively with an active trading strategy.

\newpage

### 3.4. Single Simple Moving Average Trading

Next thing to look at is the optimization with a single simple moving average. For AR models, the start date is a hyperparameter, as is the model order *p*. For SMA models there is no model order but a filter length *L*. Limit the filter length here to $5<L<500$, since the out-of-sample range is only 350 days long.

Additionally the in-sample range is varied again. For each start date the in-sample performances are calculated for each filter length in the interval $5<L<500$. The AR model was only limited to the performance indicator sharpe ratio. This time we are adding the drawdown and MSE indicators. Thus, for each start date, for each filter length *L*, the three different indicators are calculated and an optimal model can be choosen.

#### 3.4.1. Optimization

&nbsp;

The following figure \ref{fig:chap3.2.3.1} visually represents the complete SMA optimization of the first index. The two performance indicators sharpe (red) and MSE (green) correspond to the left y-axis. The drawdown (blue) was scaled to a visually appealing size for comparison and corresponds to the right y-axis. The x-axis shows the respective in-sample start dates, which calculates the respective performance indicators Sharpe, drawdown and MSE from filter lengths 5 to 500. The bold dots in the respective sections mark the optimal values of the respective indicator.

```{r, chap3.2.3.1, echo=FALSE, fig.cap="Optimization Index 1.", fig.dim=c(11, 6), out.width = "100%", fig.keep = 'last'}
seper <- (500-5)+1
x_s <- res1$res_df[,3] + 1 + (0:13*seper)
y_s <- res1$res_df[,2]

x_d <- res1$res_df[,5] + 1 + (0:13*seper)
y_d <- res1$res_df[,4]

x_m <- res1$res_df[,7] + 1 + (0:13*seper)
y_m <- res1$res_df[,6]

plot(x=1:length(res1$lirino[,2]), y=res1$lirino[,2], type="l", col="#DF536B", main="Performance Indicators | SMA | Index 1", xlab = "L", ylab="", xaxt = "n", ylim=c(-2, 3))
axis(1, at=c(1, seper, 1+2*seper, 2*seper+seper), labels=c("5", "500", "5", "500"))
axis(3, at=c(1+seper, seper+seper), labels=c("5", "500"))
axis(4, at=c(-0.5, 0.5, 1.5), labels=c("-0.10", "-0.05", "0"))

abline(v=1, lty=2, col="black")
abline(v=1+(500-5), lty=2, col="black")
abline(h=1.5, lty=2, col="black")
abline(h=0, lty=2, col="black")

lines((res1$lirino[,3])*18.23+1.5, type="l", col="#2297E6")
lines(res1$lirino[,4], type="l", col="#61D04F")

points(x_s, y_s, pch=19, col="#DF536B")
points(x_d, y_d*18.23+1.5, pch=19, col="#2297E6")
points(x_m, y_m, pch=19, col="#61D04F")

legend("topleft", legend=c("Sharpe", "MSE"),
       col=c("#DF536B", "#61D04F"), lty=c(1,1), cex=0.8,
       bty = "n")

legend("topright", legend="Drawdown",
       col="#2297E6", lty=1, cex=0.8,
       bty = "n")

for (i in 0:13) {
  text((1+500-5)/2 + seper*i, -2, paste(2003+i, "-01-01", sep=""), cex=0.6)
}
```

It is immediately noticeable that in each time window the trajectories are very repetitive. If we consider a single time window (2003), we can see that the Sharpe gets better for longer filters. Towards the end of the Sharpe optimization, the amplitude increases and diverges towards the end. With the drawdown this behaves exactly the opposite. It is noticeable with the drawdown, however, that with short filters the values fluctuate very strongly, which is not the case with long filters. What is also noticeable is that especially towards the middle (from 2007) the drawdown seems to level off, but in this time window several points could point to the optimal value. This randomness is discussed in more detail in section [3.5](#macrossingse). Already after a few iteration steps, the MSE seems to stabilize. It is difficult to find an optimum here, because practically all values are very similar.

\newpage

Table \ref{tab:restab1} shows the numerically best values for the respective start date and performance indicators of the optimization. The marked values should indicate the best filter lengths in each case. What is also striking here is that the Sharpe filter lengths from 2003 to 2013 are all identical. To describe this phenomenon further, a deeper analysis would be necessary, but this is not described further here.

The optimal filter lengths for the corresponding indicators are now used to trade out-of-sample by sign in figure \ref{fig:chap3.2.3.2}.

&nbsp;


```{r restab1, echo=FALSE}
tab1 <- res1$res_df
tab1$Sharpe <- round(tab1$Sharpe, 2)
tab1$Drawdown <- round(tab1$Drawdown, 6)
tab1$MSE <- round(tab1$MSE, 3)

kbl(tab1, caption="SMA-Optima by performance indicators | Index 1.", booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  column_spec(2:3, color = "#DF536B",
              background = c(rep("#FFFFFF", 13), "#F4DBDF")) %>%
  column_spec(4:5, color = "#2297E6",
              background = c(rep("#FFFFFF", 13), "#C2DFF5")) %>%
  column_spec(6:7, color = "#61D04F",
              background = c(rep("#FFFFFF", 5), "#C2EABC",rep("#FFFFFF", 8)))
```

&nbsp;

Note that a little trickery was used for the out-of-sample calculation. If SMA is used, the initialization phase of the filters must be taken into account. For example, for an SMA with a filter length of L=50, an initialization phase of 50 time units would have to be considered. Here the in-sample ranges were also used in order to be able to represent the comparability of the filters better. In practice, reliable trading signals could only be expected after the initialization phase.

\newpage

Applying the filter lengths of the best indicators to index 1, the following out-of-sample solutions are obtained \ref{fig:chap3.2.3.2}. For the MSE (green), a very long filter L=486 was chosen as the optimal filter. This reacts very late to fluctuations in the time-series. The index 1 shows a constantly rising trend, which leads to the fact that the MSE filter does not signal any trades.

Sharpe and drawdown lead to the exact same filter length of L=69 with the same in-sample range with start date 2016-01-01. At the beginning of December 2019 a trade is signaled, which leads to a decline in performance.

With none of the found optimal filter lengths and in-sample ranges the passive buy-and-hold strategy could be outperformed.

&nbsp;

```{r, chap3.2.3.2, echo=FALSE, fig.cap="Applying optimal filterlengths to the Index 1.", fig.dim=c(11, 6), out.width = "100%"}
perfplot_sma_opt(xall=log_ret_ind, insamp="2019-01-01", res_obj=res1, inx=1)
```

Applying the optimization algorithm to the other 3 time-series leads to similar results, which can be seen in the appendix [6.3.4.2](#sma-appendix). It seems almost impossible to outperform the highly praised buy-and-hold strategy with an SMA.

\newpage

#### 3.4.2. Out-of-Sample Optimization

&nbsp;

Different in-sample lengths have been used to try to find an optimal in-sample model, which should lead to satisfying solutions in the out-of-sample area. This was not possible with the methods used. Now one would like to examine additionally whether it would be at all possible to find good solutions in the out-of-sample range. Therefore we investigate the out-of-sample area directly. For the entire runtime of the time-series, filtered SMAs are generated for all filter lengths $5<L<500$. However, one considers only the out-of-sample range for the analysis. One limits oneself here to the performance indicator Sharpe and calculates this for each *L*.

Note, however, that this is only possible here because you know the real values of the out-of-sample range. If this procedure finds filter lengths that outperform buy-and-hold, these cannot be applied in reality. The filters could only be found by the out-of-sample data, which we do not know in reality. Here, we only investigate whether it is at all possible to outperform buy-and-hold.

In figure \ref{fig:chap3.2.3.3}, one can see the results of the out-of-sample Sharpe ratios of the different filter lengths of the 4 indexes. The dashed horizontal lines reflects the buy-and-hold Sharpe of the indexes. The dots indicate Sharpe ratios that have a higher value than buy-and-hold, i.e. filter lengths that lead to an outperformance.

```{r, chap3.2.3.3, echo=FALSE, fig.cap="Out-of-sample Sharpe for each Index.", fig.dim=c(11, 6), out.width = "100%"}
bnh_sharpe_1 <- sharpe_fun(log_ret_ind[, 1]["2019-01-01/"])
bnh_sharpe_2 <- sharpe_fun(log_ret_ind[, 2]["2019-01-01/"])
bnh_sharpe_3 <- sharpe_fun(log_ret_ind[, 3]["2019-01-01/"])
bnh_sharpe_4 <- sharpe_fun(log_ret_ind[, 4]["2019-01-01/"])

ind1_outperf <- oof_sma_opt_1[oof_sma_opt_1$Sharpe > bnh_sharpe_1, ]
ind2_outperf <- oof_sma_opt_2[oof_sma_opt_2$Sharpe > bnh_sharpe_2, ]
ind3_outperf <- oof_sma_opt_3[oof_sma_opt_3$Sharpe > bnh_sharpe_3, ]
ind4_outperf <- oof_sma_opt_4[oof_sma_opt_4$Sharpe > bnh_sharpe_4, ]

# AIO-Sharope####
plot(oof_sma_opt_1$Sharpe, type="l", main="SMA-Sharpe-Optimization", ylab="Sharpe Ratio", xlab="L", xaxt="n", col="#000000", ylim=c(-0.5, 3.7))
axis(1, at=c(1, 96, 196, 296, 396, 496), labels=c("5", "100", "200", "300", "400", "500"))
points(ind1_outperf$L-4, ind1_outperf$Sharpe, pch=19, col="#000000")
abline(h=bnh_sharpe_1, lty=2,col="#000000")
legend("bottomright", legend=c("Index 1", "Index 2", "Index 3", "Index 4"),
       col=c("#000000", "#DF536B", "#61D04F", "#2297E6"), lty=c(1,1, 1, 1), cex=0.8,
       bty = "n")

abline(h=bnh_sharpe_2, lty=2,col="#DF536B")
lines(oof_sma_opt_2$Sharpe, type="l", col="#DF536B")
points(ind2_outperf$L-4, ind2_outperf$Sharpe, pch=19, col="#DF536B")

abline(h=bnh_sharpe_3, lty=2,col="#61D04F")
lines(oof_sma_opt_3$Sharpe, type="l", col="#61D04F")
points(ind3_outperf$L-4, ind3_outperf$Sharpe, pch=19, col="#61D04F")

abline(h=bnh_sharpe_4, lty=2,col="#2297E6")
lines(oof_sma_opt_4$Sharpe, type="l", col="#2297E6")
points(ind4_outperf$L-4, ind4_outperf$Sharpe, pch=19, col="#2297E6")

abline(v=1, lty=2, col="#E413A3")
abline(v=119, lty=2, col="#E413A3")

abline(v=280, lty=2, col="#14C29A")
abline(v=446, lty=2, col="#14C29A")

text((119-1)/2, -0.2, "Phase 1", col="#E413A3")
text(((446-280)/2)+280, -0.2, "Phase 2", col="#14C29A")

```
Here one can see again a typical characteristic of these time-series. In chapter [3.1.2](#vola-sharpe) we compared the volatility with the Sharpe ratios, here we can see this phenomenon well. The index 1 leads to higher Sharpe ratios than for example the index 4, but is less volatile. The Sharpe ratio of index 4 shows very strong volatile phases in comparison.

Up to a filter length of about 40, the Sharpe ratios fluctuate very strongly. This is followed by a volatile phase until the filter Sharpe converges to the buy-and-hold Sharpe (from about L=120). At high filter lengths the Sharpe ratios for indexes 2,3 and 4 start to fluctuate again, the more volatile the original index is, the earlier the volatile phase starts and the longer it is. What is noticeable is that there are 2 phases which leads to good models. On the one hand you can find SMA's with short filters (phase 1) and on the other hand with long ones (phase 2). In both phases models are found, which can lead to an outperformance (they are above the dashed buy-and-hold line).

\newpage

In the following table \ref{tab:tab-oof}, the marked Sharpe ratios are shown with their corresponding filter lengths. Additionally, the trades signaled by the filtered series are listed. The first row corresponds to the buy-and-hold. The respective phases have been visualized in color.

```{r tab-oof, echo=FALSE}
tab1 <- ind1_outperf
tab1$Sharpe <- round(tab1$Sharpe, 3)
tab2 <- ind2_outperf
tab2$Sharpe <- round(tab2$Sharpe, 3)
tab3 <- ind3_outperf
tab3$Sharpe <- round(tab3$Sharpe, 3)
tab4 <- ind4_outperf
tab4$Sharpe <- round(tab4$Sharpe, 3)

while (dim(tab1)[1] != 13) {
  tab1 <- rbind(tab1, c("", "", ""))
}

while (dim(tab2)[1] != 13) {
  tab2 <- rbind(tab2, c("", "", ""))
}

while (dim(tab3)[1] != 13) {
  tab3 <- rbind(tab3, c("", "", ""))
}

while (dim(tab4)[1] != 13) {
  tab4 <- rbind(tab4, c("", "", ""))
}
# BnH
tab1 <- rbind(c(1, 1, round(bnh_sharpe_1, 3)), tab1)
tab2 <- rbind(c(1, 1, round(bnh_sharpe_2, 3)), tab2)
tab3 <- rbind(c(1, 1, round(bnh_sharpe_3, 3)), tab3)
tab4 <- rbind(c(1, 1, round(bnh_sharpe_4, 3)), tab4)

tab1["|"] <- rep("|", 14)
tab2["|"] <- rep("|", 14)
tab3["|"] <- rep("|", 14)


df_tab <- cbind(tab1, tab2, tab3, tab4)
rownames(df_tab) <- NULL

kbl(df_tab, caption="Best ouf-of-sample filterlengths.", booktabs = T, linesep = "") %>%
  add_header_above(c("Index 1"=3, " "=1, "Index 2"=3, " "=1, "Index 3"=3, " "=1, "Index 4"=3)) %>%
  kable_styling(latex_options = c("hold_position")) %>%
  row_spec(1, background="#F4F4F4") %>%
  column_spec(1:3, color = c("#000000", rep("#E413A3", 2), rep("#000000", 11))) %>%
  column_spec(5:7, color = c("#000000", rep("#E413A3", 4), rep("#14C29A", 4), rep("#000000", 5))) %>%
  column_spec(9:11, color = c("#000000", rep("#E413A3", 6), rep("#14C29A", 7))) %>%
  column_spec(13:15, color = c("#000000", rep("#E413A3", 6), rep("#14C29A", 6), "#000000")) 
```

&nbsp;

The short filters (pink) lead to more trades compared to the long filters (green). This also makes sense, short filters react faster and tend to lead to a higher number of trading signals. Some models found are also only marginally better than buy-and-hold, some would lose relevance when trading costs are taken into account.

Lastly, one would just like to investigate how the generated signals of the short and long filters differ. In the following figure \ref{fig:chap3.2.3.4} two models with different filter lengths of index 2 are shown as an example.

\newpage

The two chosen filters are the most optimal for index 2. Both lead to a similar result. Over the entire out-of-sample period, the long filter (green) generates a single signal in mid-January 2019, which leads to an outperformance. The short filter (pink) generates a signal in March 2019 that leads to nothing. Only towards the end of 2019 to the beginning of 2020 several consecutive trading signals are generated, which finally lead to a similar result as the long filter.

&nbsp;

```{r, chap3.2.3.4, echo=FALSE, fig.cap="Two good models | Index 2.", fig.dim=c(11, 6), out.width = "100%"}
plot_sma2(x=log_ret_ind, inx=2, L1=42, L2=354, insamp = "2018-12-31")
```

&nbsp;

Thus, it would be proven that buy-and-hold can be outperformed with a single simple moving average. However, this could only be done by meeting some conditions. The optimal filters could only be found by knowing the out-of-sample data, which is not known in reality. If the used time-series would take a different course, the solutions with the used filters could look completely different.

\newpage

### 3.5. Moving Average Crossings Trading {#macrossingse}
 
As a third approach we apply 2 SMAs on every time-series and trade them with the common Ma crossings rules introduced in [2.3.3](#macross-section).
With an optimization we are trying to find the best filterlengths for the 2 SMAs.

#### 3.5.1. Optimization

&nbsp;

For the optimization the same procedure as in the previous sections gets executed.

The optimization has two levels, the "first level" or outer level is  narrowing the in-sample timespan  from (2003-01 / 01-2019) to (01-2018 / 01-2019). In the inner level "second level" the real optimization is calculated by finding the optimal combination of filters, regarding maximum Sharpe and maximum drawdown trading them with the common crossing trading rules.

Hyperparameter:

- $\textnormal{Date}$ = startdate for the in-sample timespan $\textnormal{2003-01-01} < \textnormal{date} \le \textnormal{2018-01-01}\textnormal{, by 1 trading year}$

Parameters$_2$:

- $L1$ = length of shorter SMA  with $1 < L1 \le  50$ 
- $L2$ = length of longer  SMA with  $100 < L1 \le 250$

Figure \ref{fig:chap3.2.4.1.1} visualizes one iteration of the optimization. For the fixed L1=1, L2 is variable and the optimal sharpe is calculated. This procedure is done for every filterlength as described in $_2$ and further visualized in figure \ref{fig:chap3.2.4.1.2}. The drawdown is also calculated as seen in plot \ref{fig:chap3.2.4.1.3}. Notice that the visualization is from the starting date 2015-10-30, which is randomly chosen for explaining the optimization process.


```{r, chap3.2.4.1.1, echo=FALSE, fig.cap="Optimization Sharpe L1=1 ,L2=variable.", include=T,out.width = "100%",fig.dim=c(16, 7.4)}
sharpfirst=as.array((ind1_opt$perf_mat)[,2])
plot(sharpfirst[1:151],type="l",ylab="Max sharpe",xlab="L1 fixed=1,    100 < L2 < 250 on x axis ",main="Filter optimization start 2015  ",col="black",xaxt='n')
abline(v=7, col = "green")
abline(h=1.225, col = "green")
abline(v=c(0,150), col = "blue",lty=2)
legend("bottom", inset=.02, legend=c("Sharpe", "Maximum by filter L1= 1, L2 = 106","Boundaries trajectorie L1=1"),
       col=c("black", "green","blue"), lty=c(1,1,2), cex=0.8, horiz=F)
```  
\newpage
```{r, chap3.2.4.1.2, echo=FALSE, fig.cap="Sharpe Index 1.", include=T,out.width = "100%",fig.dim=c(16, 7.4)}
plot((ind1_opt$perf_mat)[,2],type="l",ylab="Sharpe",xlab="",main="Filter optimization start: 2015 ",xaxt='n')
abline(h=1.08, col = "red")
abline(h=1.225, col = "green")
abline(v=c(0,150),col="blue",lty=2)
legend("bottom", inset=.02, legend=c("Sharpe", "Maximum by filter L1= 1, L2 = 106","Boundaries trajectorie L1=1"),
       col=c("black", "green","blue"), lty=c(1,1,2), cex=0.8, horiz=F)

```
&nbsp;

```{r, chap3.2.4.1.3, echo=FALSE, fig.cap="Drawdown Index 1.", include=T,out.width = "100%",fig.dim=c(16, 7.4)}
plot((ind1_opt$perf_mat)[,3],type="l",ylab="Drawdown",xlab="",main="Filter optimization start: 2015 ",col="green")
abline(h=-0.0069, col = "violet")
abline(v=17*150)
abline(v=c(16*150-20,20 * 150+20),col="darkblue",lty=2)
legend("bottom", inset=.02, legend=c("Drawdown", "Maximum by filter L1= 18, L2 = 102","Cluster"),
       col=c("green", "black","darkblue"), lty=c(1,1,2), cex=0.8, horiz=F)
```

The pattern we observe in figure \ref{fig:chap3.2.4.1.2} is the repeating optimization by fixing L1 and vary L2. The maximum Sharpe was found with L1=*1*, L2=*106*, Sharpe=*1.225*, which is also visualized in figure \ref{fig:chap3.2.4.1.2}, where the "window" (blue dotted lines) is shown. When we look at the red line we can clearly see there are other similar high spikes in other filter lengths. The green pattern in figure \ref{fig:chap3.2.4.1.3} shows interesting news. At L1 = 17, 18, 19 and 20 we see a cluster(blue dotted lines) of high spikes, all very similar. The optimum in this plot is by L1=*18* and L2=*102* and a drawdown *6.55e-3*, laying in the described cluster. But when we look at the violet line drawn, we can clearly see there is another high spike at the beginning of the plot. This spike is identical with the maximum Sharpe in figure \ref{fig:chap3.2.4.1.2}. So these results could just be random and the solution is not finite, this problem will be further discussed$_3$.

\newpage

```{r, chap3.2.4.1.4, echo=FALSE, fig.cap="Hyperoptimization Index 1.", include=T,fig.keep = 'last',fig.dim=c(16, 7.1),out.width = "100%"}
##################sharpe maxdrow
plot(index_1_onebest_year_with15[,1],ylim= c(-2,4),main= "Max Sharpe/Drawdown Hyperoptimization Index 1",col="black",type="b")
lines(index_1_onebest_year_with15[,4]*100,lwd=2,col="green",type="b")
addLegend("topleft", on=1, legend.names = c("Sharpe ", "MaxDrawdown * 100 "), lty=c(1, 1), lwd=c(2, 2),col=c("black","green"))


events <- xts("Figure 24/25/26", as.Date("2015-10-30"))
addEventLines(events,srt=90,pos=2,lty=2,)
```
&nbsp;

```{r, chap3.2.4.1.5, echo=FALSE, fig.cap="Hyperoptimization Index 1.", include=T,fig.keep = 'last',fig.dim=c(16, 7.1),out.width = "100%"}
################## lengths
plot(index_1_onebest_year_with15[,2],type="b",col= "red",main= "Filterlengths Hyperoptimization Index 1 ",ylim=c(-10,260))

lines(index_1_onebest_year_with15[,3], col=c("coral"),lwd=2,type="b")
lines(index_1_onebest_year_with15[,5], col=c("blue"),lwd=2,type="b")
lines(index_1_onebest_year_with15[,6], col=c("cornflowerblue"),lwd=2,type="b")

addLegend("left", on=1, legend.names = c("long filter sharpe", "long filter drawdown "), lty=c(1, 1), lwd=c(1, 1),col=c("coral","cornflowerblue"))

addLegend("bottomleft", on=1, legend.names = c("short filter sharpe", "short filter Drawdown "), lty=c(1, 1), lwd=c(1, 1),col=c("red","blue"))

addEventLines(events,srt=90,pos=2,lty=2)

```

By plotting the Sharpe ratio / max drawdown$_1$ in figure \ref{fig:chap3.2.4.1.4} and the optimal filterlenghts L1/L2 in figure \ref{fig:chap3.2.4.1.5}, for every in-sample timestep as described at the beginning of [3.5.1](#macrossingse), the hyperoptimization is visualized. The code is linked in the attachement [6.3.5](#optimizationmacross). The results from the trajectories in figure \ref{fig:chap3.2.4.1.2} and figure \ref{fig:chap3.2.4.1.3}
are showed at the black dotted line. Because of the problem we have mentioned in $_3$ the best result could also be just random. Therefore the ten best results for each in-sample timesteps are calculated and visualized in figures \ref{fig:chap3.2.4.1.6} and \ref{fig:chap3.2.4.1.7} to get an idea about the accuracy of these filters and analyze them a bit further.

$_1$ Note: maxdrawdown is scaled by the factor 100 to visualize it in the same plot.
\newpage
```{r, chap3.2.4.1.6, echo=FALSE, fig.cap="Hyperoptimization 10 best Index 1.", include=T,fig.keep = 'last',fig.dim=c(16, 7.1),out.width = "100%"}
##################sharpe maxdrow 10
plot(index_1_tenbest_year[,1],ylim= c(-2,4),main= "Max Sharpe/Drawdown 10 best Hyperoptimization Index 1",col="black",type="p",major.ticks="auto",format.labels="%b-%Y")
lines(index_1_tenbest_year[,4]*100,lwd=2,col="green",type="p")
addLegend("topleft", on=1, legend.names = c("Sharpe ", "MaxDrawdown * 100 "), lty=c(1, 1), lwd=c(2, 2),col=c("black","green"))

```

```{r, chap3.2.4.1.7, echo=FALSE, fig.cap="Hyperoptimization 10 best Index 1.", include=T,fig.keep = 'last',fig.dim=c(16, 7.1),out.width = "100%"}
################## lengths
plot(index_1_tenbest_year[,2],type="p",col= "red",main= "Filterlengths 10 best Hyperoptimization Index 1  ",ylim=c(-10,260),format.labels="%b-%Y")
lines(index_1_tenbest_year[,3], col=c("coral"),lwd=2,type="p")
lines(index_1_tenbest_year[,5], col=c("blue"),lwd=2,type="p")
lines(index_1_tenbest_year[,6], col=c("cornflowerblue"),lwd=2,type="p")

addLegend("left", on=1, legend.names = c("long filter sharpe", "long filter drawdown "), lty=c(1, 1), lwd=c(1, 1),col=c("coral","cornflowerblue"))
addLegend("bottomleft", on=1, legend.names = c("short filter sharpe", "short filter Drawdown "), lty=c(1, 1), lwd=c(1, 1),col=c("red","blue"))

```

Whilst the ten best Sharpe ratios / drawdowns in figure \ref{fig:chap3.2.4.1.6} are always very close together, the filterplot \ref{fig:chap3.2.4.1.6}  shows a completely different and interesting image. We can observe differences, specially in the long filters in 2009, 2010, 2011 the result nearly diverge 150 units. Also in the timespan 2011 to 2018 the filters fluctuate strongly. What further catches the eye, are the long- and the shortfilter sharpes from 2003 to 2011. They are all concentrated at their upper limits of 50 and 250, which indicates that they actually have to be longer than the restriction. The same behaviour is observed for the short drawdown from 2011 to 2018 where the observations are concentrated around length 1. The plot would surely look different if the restrictions of the filterlengths would be greater. Also the steps of the in-sample timespan are always one year. Changing the restriction would have a massive impact on the optimization time and would most likely deliver different results.

Curiosity leads us to recreate the optimization for $1 < L1 \le 1000$ and $1 < L2 \le 1000$  which calculates 16 million Sharpes.  Now L2 can be smaller than L1 while the trading rule stays the same. The numeric results can be found in table \ref{tab:macrosstable 1000 x 1000 index 4.1}.

\newpage

```{r, chap3.2.4.1.14, echo=FALSE, fig.cap="Hyperoptimization 1000 x 1000 Index 4.", include=T,fig.keep = 'last',fig.dim=c(16, 7.1),out.width = "100%"}
plot(rolf1[,1],ylim= c(-2,4),col="black",type="p",format.labels="%b-%Y",main="Optimization with 1000x1000 index 4")
addLegend("topleft", on=1, legend.names ="Sharpe " ,lty=1, lwd=2,col="black")
```

```{r, chap3.2.4.1.15, echo=FALSE, fig.cap="Hyperoptimization 10 best Index 4.", include=T,fig.keep = 'last',fig.dim=c(16, 7.1),out.width = "100%"}
plot(rolf1[,2],type="p",col= "red",main= "Optimization with 1000x1000 index 4 ",ylim=c(-200,1010),format.labels="%b-%Y")
lines(rolf1[,3], col=c("blue"),lwd=2,type="p")
addLegend("left", on=1, legend.names = c("long filter sharpe", "long filter drawdown "), lty=c(1, 1), lwd=c(1, 1),col=c("red","blue"))
```

\newpage

In figure \ref{fig:chap3.2.4.1.14} the Sharpes of the 10 best models look all very similar in each step except for 2009 and 2012 they vary a little bit. Until 2015 they are all slightly under 1. Very important is the fact that a filterlength of 100 means 4 years which is a very long time. The results narrowing Jan-2019 are close to or directly buy-and-hold, therefore the sharpe rises to nearly 3. Interesting are also the result in figure \ref{fig:chap3.2.4.1.15}. From 2002 to 2011 the long and the short filter are always very close together. This plot could lead to a more interesting analysis. Due the time and length limitation this optimization is closed here and not further discussed.


In good knowledge we are taking the risk of maybe not finding the right filterlength and use the result of the optimizations for trading the series in the next section.







\newpage
#### 3.5.2. Trading
&nbsp;

As noticed the Visualizations in [3.5.1](#macrossingse) are only done for index 1, the optimal values and their fluctuation are shown in the tables: \ref{tab:macrosstable index 1}, \ref{tab:macrosstable index 2}, \ref{tab:macrosstable index 3}, \ref{tab:macrosstable index 4} and figures:  \ref{fig:chap3.2.4.1.8}, \ref{fig:chap3.2.4.1.9}, \ref{fig:chap3.2.4.1.10}, \ref{fig:chap3.2.4.1.11}, \ref{fig:chap3.2.4.1.12}, \ref{fig:chap3.2.4.1.13}.
```{r, chap3.5.2.1. , echo=FALSE, include=T }
return_buy_and_hold_1  =   diff(log(data[,1]))##### buy-and-hold
return_buy_and_hold_2  =   diff(log(data[,2]))
return_buy_and_hold_3  =   diff(log(data[,3]))
return_buy_and_hold_4  =   diff(log(data[,4]))
```
Based on the optimal filterlenghts calculated in the last section, the SMAs are now applied to the "out-of-sample" span from 2019-01-01 to 2020-04-01. Keeping in mind that the Moving Averages need the same initialization time as the length of filter itself, only the optimal filterlengths from years smaller than 2017 are considered.
Index 1 brings us the optimal maximum Sharpe *1.275* at end of the series *2016* with filterlengths *L1=1, L2=106* applying this filters to the put opf sample leads us to buy-and-hold no trades are executed. Even considering the filters from the earlier timesspans *2006-01-01, Sharpe=1.529, L1=49, L2=243*  brings no other result than buy-and-hold. Trying the maximum drawdown filters leads to the same result. 

&nbsp;

```{r, chap3.5.2.2. , echo=FALSE, fig.cap="MA Crossing index 1, L1 = 1, L2 = 106.", include=T, fig.keep = 'last',out.width="100%"}
n1=1
n2 =106
mobidat= data[,1] # chose the row and horizon
start="2019-01-01"
end = "2020-04-31"
horizon=paste(start,"::",end,sep = "")
sma1 <-SMA(mobidat,n=n1)
sma2 <-SMA(mobidat,n=n2)
signal <-rep(0,length(sma1))
signal[which(sma1>sma2&lag(sma1)<lag(sma2))]<-1
signal[which(sma1<sma2&lag(sma1)>lag(sma2))]<--1
signal[which(sma1>sma2)]<-1
signal[which(sma1<sma2)]<--1
signal=reclass(signal,sma1)
chartSeries(mobidat,subset=horizon,theme=chartTheme("white", bg.col="#FFFFFF"),name= "SMA",type="")
addSMA(n=n1,on=1,col = "blue")
addSMA(n=n2,on=1,col = "red")
addTA(signal,type="S",col="red")
trade   =   Lag(signal[horizon],1)
return  =   diff(log(mobidat))
ret = return*trade
names(ret)="filter"
```
\newpage
Regarding the other indexes, the results  look all very similiar. None of the Moving Average Crossing are able to outperform buy-and-hold. For visualizing we use only the fourth index with *2016-01-01*, Sharpe=0.998, L1=12, L2=134. As seen in figure \ref{fig:chap3.5.2.3} we have leastways two trades for this index. Following the longer filter (red) from the left side, it crosses the blue line (L1) in the beginning of 2020, that is where a selling is executed. Shortly after, the buy signals comes with the red line crossing the blue from above. With this signals we get a Sharpe of *1.761906*, in comparison to the buy-and-hold Sharpe index 4: *1.963966*.

&nbsp;

```{r, chap3.5.2.3, echo=FALSE, fig.cap="MA Crossing index 4, L1 = 12, L2 = 134.", include=T, fig.keep = 'last',out.width="100%"}
n1=12
n2 =134
mobidat= data[,4] # chose the row and horizon
start="2019-01-01"
end = "2020-04-31"
horizon=paste(start,"::",end,sep = "")
sma1 <-SMA(mobidat,n=n1)
sma2 <-SMA(mobidat,n=n2)
signal <-rep(0,length(sma1))
signal[which(sma1>sma2&lag(sma1)<lag(sma2))]<-1
signal[which(sma1<sma2&lag(sma1)>lag(sma2))]<--1
signal[which(sma1>sma2)]<-1
signal[which(sma1<sma2)]<--1
signal=reclass(signal,sma1)
chartSeries(mobidat,subset=horizon,theme=chartTheme("white", bg.col="#FFFFFF"),name= "SMA",type="")
addSMA(n=n1,on=1,col = "blue")
addSMA(n=n2,on=1,col = "red")
addTA(signal,type="S",col="red")
trade   =   Lag(signal[horizon],1)
return  =   diff(log(mobidat))
ret = return*trade
names(ret)="filter"
#charts.PerformanceSummary(ret, main="Naive Buy Rule")
#sqrt(255)*SharpeRatio(ret,FUN="StdDev")  # 1.761906 L1=12 ,L2 =134
```

&nbsp;

Also neither of the results from figure \ref{fig:chap3.2.4.1.15} are clearly satisfying.

As endresult buy-and-hold stays better as the combinations of MA Crossigs, at least with the parameter restrictions we implied. The intuition, and also the common used filters L1=50, L2=250 equals buy-and-hold. This strategy may not be so bad, why leaving a trend market instead of profiting from it.
